{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c821e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForPreTraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd12a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9136376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'NUM_WORKERS':4,\n",
    "    'ANTIGEN_WINDOW':256,\n",
    "    'ANTIGEN_MAX_LEN':256, # ANTIGEN_WINDOW와 ANTIGEN_MAX_LEN은 같아야합니다.\n",
    "    'EPITOPE_MAX_LEN':256,\n",
    "    'EPOCHS':50,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':128,\n",
    "    'THRESHOLD':0.7,   # 기본적으로 0.5로 사용하지만 data impalance가 심할 경우 더 큰 값을 사용하기도 한다.\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3a43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "847b9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('model/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a1240c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('[PAD]', 0),\n",
       "             ('[UNK]', 1),\n",
       "             ('[CLS]', 2),\n",
       "             ('[SEP]', 3),\n",
       "             ('[MASK]', 4),\n",
       "             ('L', 5),\n",
       "             ('A', 6),\n",
       "             ('G', 7),\n",
       "             ('V', 8),\n",
       "             ('E', 9),\n",
       "             ('S', 10),\n",
       "             ('I', 11),\n",
       "             ('K', 12),\n",
       "             ('R', 13),\n",
       "             ('D', 14),\n",
       "             ('T', 15),\n",
       "             ('P', 16),\n",
       "             ('N', 17),\n",
       "             ('Q', 18),\n",
       "             ('F', 19),\n",
       "             ('Y', 20),\n",
       "             ('M', 21),\n",
       "             ('H', 22),\n",
       "             ('C', 23),\n",
       "             ('W', 24),\n",
       "             ('X', 25),\n",
       "             ('U', 26),\n",
       "             ('B', 27),\n",
       "             ('Z', 28),\n",
       "             ('O', 29)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc7e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqtoinput(seq):\n",
    "    for j in range(len(seq)-1):\n",
    "        seq = seq[:j+j+1]+ ' ' + seq[j+j+1:]\n",
    "    return seq\n",
    "    \n",
    "\n",
    "def get_preprocessing(data_type, new_df, tokenizer):\n",
    "    epitope_ids_list = []\n",
    "    epitope_mask_list = []\n",
    "    \n",
    "    left_antigen_ids_list = []\n",
    "    left_antigen_mask_list = []\n",
    "    \n",
    "    right_antigen_ids_list = []\n",
    "    right_antigen_mask_list = []\n",
    "        \n",
    "    for epitope, antigen, s_p, e_p in tqdm(zip(new_df['epitope_seq'], new_df['antigen_seq'], new_df['start_position'], new_df['end_position'])):        \n",
    "        # Left antigen : [start_position-WINDOW : start_position]\n",
    "        # Right antigen : [end_position : end_position+WINDOW]\n",
    "\n",
    "        start_position = s_p-CFG['ANTIGEN_WINDOW']-1\n",
    "        end_position = e_p+CFG['ANTIGEN_WINDOW']\n",
    "        if start_position < 0:\n",
    "            start_position = 0\n",
    "        if end_position > len(antigen):\n",
    "            end_position = len(antigen)\n",
    "        \n",
    "        # left / right antigen sequence 추출\n",
    "        left_antigen = antigen[int(start_position) : int(s_p)-1]\n",
    "        \n",
    "        right_antigen = antigen[int(e_p) : int(end_position)]\n",
    "\n",
    "        if CFG['EPITOPE_MAX_LEN']<len(epitope):\n",
    "            epitope = epitope[:CFG['EPITOPE_MAX_LEN']]\n",
    "        else:\n",
    "            epitope = epitope[:]\n",
    "        \n",
    "        left_antigen = seqtoinput(left_antigen)\n",
    "        right_antigen = seqtoinput(left_antigen)\n",
    "        epitope = seqtoinput(epitope)\n",
    "        \n",
    "        if len(left_antigen) == 0:\n",
    "            left_antigen = '[PAD]'\n",
    "            \n",
    "        if len(right_antigen) == 0:\n",
    "            right_antigen = '[PAD]'\n",
    "        \n",
    "        left_antigen_input = tokenizer(left_antigen, add_special_tokens=True, pad_to_max_length=True, max_length = 256)\n",
    "        left_antigen_ids = left_antigen_input['input_ids']\n",
    "        left_antigen_mask = left_antigen_input['attention_mask']\n",
    "        \n",
    "        right_antigen_input = tokenizer(right_antigen, add_special_tokens=True, pad_to_max_length=True, max_length = 256)\n",
    "        right_antigen_ids = right_antigen_input['input_ids']\n",
    "        right_antigen_mask = right_antigen_input['attention_mask']\n",
    "        \n",
    "        epitope_input = tokenizer(epitope, add_special_tokens=True, pad_to_max_length=True, max_length = 256)\n",
    "        epitope_ids = epitope_input['input_ids']\n",
    "        epitope_mask = epitope_input['attention_mask']\n",
    "        \n",
    "        \n",
    "        epitope_ids_list.append(epitope_ids)\n",
    "        epitope_mask_list.append(epitope_mask)\n",
    "        \n",
    "        left_antigen_ids_list.append(left_antigen_ids)\n",
    "        left_antigen_mask_list.append(left_antigen_mask)\n",
    "        \n",
    "        right_antigen_ids_list.append(right_antigen_ids)\n",
    "        right_antigen_mask_list.append(right_antigen_mask)\n",
    "    \n",
    "    label_list = None\n",
    "    if data_type != 'test':\n",
    "        label_list = []\n",
    "        for label in new_df['label']:\n",
    "            label_list.append(label)\n",
    "    print(f'{data_type} dataframe preprocessing was done.')\n",
    "    return epitope_ids_list, epitope_mask_list, left_antigen_ids_list, left_antigen_mask_list, right_antigen_ids_list, right_antigen_mask_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a082122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "152648it [07:55, 320.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38163it [01:58, 320.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "train, val = train_test_split(train, train_size=0.8, random_state=12)\n",
    "\n",
    "train_epitope_ids_list, train_epitope_mask_list, train_left_antigen_ids_list, train_left_antigen_mask_list, train_right_antigen_ids_list, train_right_antigen_mask_list, train_label_list = get_preprocessing('train', train, tokenizer)\n",
    "val_epitope_ids_list, val_epitope_mask_list, val_left_antigen_ids_list, val_left_antigen_mask_list, val_right_antigen_ids_list, val_right_antigen_mask_list, val_label_list = get_preprocessing('val', val, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "821a001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, epitope_ids_list, epitope_mask_list, left_antigen_ids_list, left_antigen_mask_list, right_antigen_ids_list, right_antigen_mask_list, label_list):\n",
    "        self.epitope_ids_list = epitope_ids_list\n",
    "        self.epitope_mask_list = epitope_mask_list\n",
    "        self.left_antigen_ids_list = left_antigen_ids_list\n",
    "        self.left_antigen_mask_list = left_antigen_mask_list\n",
    "        self.right_antigen_ids_list = right_antigen_ids_list\n",
    "        self.right_antigen_mask_list = right_antigen_mask_list\n",
    "        self.label_list = label_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        self.epitope_ids = self.epitope_ids_list[index]\n",
    "        self.epitope_mask = self.epitope_mask_list[index]\n",
    "        \n",
    "        self.left_antigen_ids = self.left_antigen_ids_list[index]\n",
    "        self.left_antigen_mask = self.left_antigen_mask_list[index]\n",
    "        \n",
    "        self.right_antigen_ids = self.right_antigen_ids_list[index]\n",
    "        self.right_antigen_mask = self.right_antigen_mask_list[index]\n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            self.label = self.label_list[index]\n",
    "            return torch.tensor(self.epitope_ids), torch.tensor(self.epitope_mask), torch.tensor(self.left_antigen_ids), torch.tensor(self.left_antigen_mask), torch.tensor(self.right_antigen_ids), torch.tensor(self.right_antigen_mask), self.label\n",
    "        else:\n",
    "            return torch.tensor(self.epitope_ids), torch.tensor(self.epitope_mask), torch.tensor(self.left_antigen_ids), torch.tensor(self.left_antigen_mask), torch.tensor(self.right_antigen_ids), torch.tensor(self.right_antigen_mask)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.epitope_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2e82c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_epitope_ids_list, train_epitope_mask_list, train_left_antigen_ids_list, train_left_antigen_mask_list, train_right_antigen_ids_list, train_right_antigen_mask_list, train_label_list)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "\n",
    "val_dataset = CustomDataset(val_epitope_ids_list, val_epitope_mask_list, val_left_antigen_ids_list, val_left_antigen_mask_list, val_right_antigen_ids_list, val_right_antigen_mask_list, val_label_list)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae8b932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=30, # default는 영어 기준이므로 내가 만든 vocab size에 맞게 수정해줘야 함\n",
    "    hidden_size=1024,\n",
    "    num_hidden_layers=3,    # layer num\n",
    "    num_attention_heads=8,    # transformer attention head number\n",
    "    intermediate_size=4096,   # transformer 내에 있는 feed-forward network의 dimension size\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    max_position_embeddings=500,    # embedding size 최대 몇 token까지 input으로 사용할 것인지 지정\n",
    "    type_vocab_size=2,    # token type ids의 범위 (BERT는 segmentA, segmentB로 2종류)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995dea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = BertForPreTraining(config=config)\n",
    "pre.save_pretrained('model/transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b9251b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 epitope_length=CFG['EPITOPE_MAX_LEN'],\n",
    "                 epitope_emb_node=1024,\n",
    "                 epitope_hidden_dim=1024,\n",
    "                 left_antigen_length=CFG['ANTIGEN_MAX_LEN'],\n",
    "                 left_antigen_emb_node=1024,\n",
    "                 left_antigen_hidden_dim=1024,\n",
    "                 right_antigen_length=CFG['ANTIGEN_MAX_LEN'],\n",
    "                 right_antigen_emb_node=1024,\n",
    "                 right_antigen_hidden_dim=1024,\n",
    "                 pretrained_model='model/transformer'\n",
    "                ):\n",
    "        super(TransformerModel, self).__init__()              \n",
    "        # Transformer                \n",
    "        self.epitope_transformer = BertModel.from_pretrained(pretrained_model)\n",
    "        \n",
    "        self.left_antigen_transformer = BertModel.from_pretrained(pretrained_model)\n",
    "        \n",
    "        self.right_antigen_transformer = BertModel.from_pretrained(pretrained_model)\n",
    "\n",
    "        in_channels = epitope_hidden_dim+left_antigen_hidden_dim+right_antigen_hidden_dim\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.Linear(in_channels, in_channels//4),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.BatchNorm1d(in_channels//4),\n",
    "            nn.Linear(in_channels//4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, epitope_x1, epitope_x2, left_antigen_x1, left_antigen_x2, right_antigen_x1, right_antigen_x2):\n",
    "        BATCH_SIZE = epitope_x1.size(0)\n",
    "        # Get Embedding Vector\n",
    "        epitope_x = self.epitope_transformer(input_ids=epitope_x1, attention_mask=epitope_x2)[0]\n",
    "        \n",
    "        left_antigen_x = self.left_antigen_transformer(input_ids=left_antigen_x1, attention_mask=left_antigen_x2)[0]\n",
    "        \n",
    "        right_antigen_x = self.right_antigen_transformer(input_ids=right_antigen_x1, attention_mask=right_antigen_x2)[0]\n",
    "        \n",
    "        \n",
    "        # LSTM\n",
    "        epitope_hidden = epitope_x[:, 0, :]\n",
    "\n",
    "        left_antigen_hidden = left_antigen_x[:, 0, :]\n",
    "        \n",
    "        right_antigen_hidden = right_antigen_x[:, 0, :]\n",
    "        \n",
    "        # Feature Concat -> Binary Classifier\n",
    "        x = torch.cat([epitope_hidden, left_antigen_hidden, right_antigen_hidden], axis=-1)\n",
    "        x = self.classifier(x).view(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92e9092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device) \n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for epitope_ids_list, epitope_mask_list, left_antigen_ids_list, left_antigen_mask_list, right_antigen_ids_list, right_antigen_mask_list, label in tqdm(iter(train_loader)):\n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "\n",
    "            left_antigen_ids_list = left_antigen_ids_list.to(device)\n",
    "            left_antigen_mask_list = left_antigen_mask_list.to(device)\n",
    "\n",
    "            right_antigen_ids_list = right_antigen_ids_list.to(device)\n",
    "            right_antigen_mask_list = right_antigen_mask_list.to(device)\n",
    "\n",
    "            label = label.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(epitope_ids_list, epitope_mask_list, left_antigen_ids_list, left_antigen_mask_list, right_antigen_ids_list, right_antigen_mask_list)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                    \n",
    "        val_loss, val_f1 = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] Val F1 : [{val_f1:.5f}]')\n",
    "        \n",
    "        if best_val_f1 < val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.module.state_dict(), './transformer_best_model_0.7.pth', _use_new_zipfile_serialization=False)\n",
    "            print('Model Saved.')\n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c26991b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    pred_proba_label = []\n",
    "    true_label = []\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for epitope_ids_list, epitope_mask_list, left_antigen_ids_list, left_antigen_mask_list, right_antigen_ids_list, right_antigen_mask_list, label in tqdm(iter(val_loader)):\n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "\n",
    "            left_antigen_ids_list = left_antigen_ids_list.to(device)\n",
    "            left_antigen_mask_list = left_antigen_mask_list.to(device)\n",
    "\n",
    "            right_antigen_ids_list = right_antigen_ids_list.to(device)\n",
    "            right_antigen_mask_list = right_antigen_mask_list.to(device)\n",
    "\n",
    "            label = label.float().to(device)\n",
    "            \n",
    "            model_pred = model(epitope_ids_list, epitope_mask_list, left_antigen_ids_list, left_antigen_mask_list, right_antigen_ids_list, right_antigen_mask_list)\n",
    "            loss = criterion(model_pred, label)\n",
    "            model_pred = torch.sigmoid(model_pred).to('cpu')\n",
    "            \n",
    "            pred_proba_label += model_pred.tolist()\n",
    "            true_label += label.to('cpu').tolist()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    pred_label = np.where(np.array(pred_proba_label)>CFG['THRESHOLD'], 1, 0)\n",
    "    val_f1 = f1_score(true_label, pred_label, average='macro')\n",
    "    return np.mean(val_loss), val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab597eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/transformer were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at model/transformer were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at model/transformer were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1193/1193 [32:19<00:00,  1.63s/it]\n",
      "100%|██████████| 299/299 [03:45<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [0.48262] Val Loss : [0.29307] Val F1 : [0.67468]\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1193/1193 [31:53<00:00,  1.60s/it]\n",
      "100%|██████████| 299/299 [03:49<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [0.23707] Val Loss : [0.20731] Val F1 : [0.66219]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [32:04<00:00,  1.61s/it]\n",
      "100%|██████████| 299/299 [03:57<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [0.20389] Val Loss : [0.19765] Val F1 : [0.65528]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:57<00:00,  1.61s/it]\n",
      "100%|██████████| 299/299 [03:54<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train Loss : [0.19619] Val Loss : [0.20138] Val F1 : [0.68231]\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1193/1193 [32:05<00:00,  1.61s/it]\n",
      "100%|██████████| 299/299 [03:55<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train Loss : [0.19222] Val Loss : [0.72043] Val F1 : [0.63351]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:59<00:00,  1.61s/it]\n",
      "100%|██████████| 299/299 [03:53<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Train Loss : [0.19247] Val Loss : [0.20771] Val F1 : [0.61318]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:57<00:00,  1.61s/it]\n",
      "100%|██████████| 299/299 [03:40<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Train Loss : [0.18735] Val Loss : [0.19357] Val F1 : [0.67917]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:43<00:00,  1.60s/it]\n",
      "100%|██████████| 299/299 [03:48<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Train Loss : [0.18054] Val Loss : [0.19042] Val F1 : [0.65354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:48<00:00,  1.60s/it]\n",
      "100%|██████████| 299/299 [03:52<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Train Loss : [0.18243] Val Loss : [0.19426] Val F1 : [0.63657]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:48<00:00,  1.60s/it]\n",
      "100%|██████████| 299/299 [03:53<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Train Loss : [0.18787] Val Loss : [0.29320] Val F1 : [0.49975]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:37<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:52<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [11] Train Loss : [0.18510] Val Loss : [0.21133] Val F1 : [0.63704]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:41<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:52<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [12] Train Loss : [0.18575] Val Loss : [0.21055] Val F1 : [0.60759]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:34<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:33<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [13] Train Loss : [0.17957] Val Loss : [0.19421] Val F1 : [0.67397]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:45<00:00,  1.60s/it]\n",
      "100%|██████████| 299/299 [03:51<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [14] Train Loss : [0.17924] Val Loss : [0.19519] Val F1 : [0.65369]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:42<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:52<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [15] Train Loss : [0.17728] Val Loss : [0.20667] Val F1 : [0.62479]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:49<00:00,  1.60s/it]\n",
      "100%|██████████| 299/299 [03:51<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [16] Train Loss : [0.17575] Val Loss : [0.27172] Val F1 : [0.51648]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:43<00:00,  1.60s/it]\n",
      "100%|██████████| 299/299 [03:53<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [17] Train Loss : [0.17534] Val Loss : [0.19734] Val F1 : [0.64391]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:48<00:00,  1.60s/it]\n",
      "100%|██████████| 299/299 [03:52<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [18] Train Loss : [0.16924] Val Loss : [0.19327] Val F1 : [0.68117]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:41<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:33<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [19] Train Loss : [0.16899] Val Loss : [0.19307] Val F1 : [0.69067]\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1193/1193 [31:29<00:00,  1.58s/it]\n",
      "100%|██████████| 299/299 [03:50<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [20] Train Loss : [0.17044] Val Loss : [0.24252] Val F1 : [0.74798]\n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1193/1193 [31:23<00:00,  1.58s/it]\n",
      "100%|██████████| 299/299 [03:53<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [21] Train Loss : [0.17560] Val Loss : [0.29796] Val F1 : [0.57011]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:25<00:00,  1.58s/it]\n",
      "100%|██████████| 299/299 [03:49<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [22] Train Loss : [0.17135] Val Loss : [0.24256] Val F1 : [0.72384]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:12<00:00,  1.57s/it]\n",
      "100%|██████████| 299/299 [03:51<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [23] Train Loss : [0.16783] Val Loss : [0.19440] Val F1 : [0.67562]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:18<00:00,  1.57s/it]\n",
      "100%|██████████| 299/299 [03:45<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [24] Train Loss : [0.16936] Val Loss : [0.19621] Val F1 : [0.67884]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:15<00:00,  1.57s/it]\n",
      "100%|██████████| 299/299 [03:38<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [25] Train Loss : [0.16979] Val Loss : [0.20842] Val F1 : [0.73945]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:30<00:00,  1.58s/it]\n",
      "100%|██████████| 299/299 [03:50<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [26] Train Loss : [0.16935] Val Loss : [0.30247] Val F1 : [0.51627]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:25<00:00,  1.58s/it]\n",
      "100%|██████████| 299/299 [03:49<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [27] Train Loss : [0.16632] Val Loss : [0.24624] Val F1 : [0.57557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:31<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:51<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [28] Train Loss : [0.16311] Val Loss : [0.19452] Val F1 : [0.66530]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:31<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:52<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [29] Train Loss : [0.16032] Val Loss : [0.19539] Val F1 : [0.69874]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:38<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:41<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [30] Train Loss : [0.16102] Val Loss : [0.20987] Val F1 : [0.63945]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1193/1193 [31:32<00:00,  1.59s/it]\n",
      "100%|██████████| 299/299 [03:47<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [31] Train Loss : [0.15978] Val Loss : [0.24463] Val F1 : [0.54966]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 245/1193 [06:37<25:38,  1.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6695/1886794936.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCosineAnnealingLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EPOCHS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Best Validation F1 Score : [{best_score:.5f}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6695/1247110624.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TransformerModel()\n",
    "model = nn.DataParallel(model, device_ids=[1, 2, 3, 4, 5])\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10*CFG['EPOCHS'], eta_min=0)\n",
    "\n",
    "best_score = train(model, optimizer, train_loader, val_loader, scheduler, device)\n",
    "print(f'Best Validation F1 Score : [{best_score:.5f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_epitope_ids_list, test_epitope_mask_list, test_left_antigen_ids_list, test_left_antigen_mask_list, test_right_antigen_ids_list, test_right_antigen_mask_list = get_preprocessing('test', test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb4ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_epitope_ids_list, test_epitope_mask_list, test_left_antigen_ids_list, test_left_antigen_mask_list, test_right_antigen_ids_list, test_right_antigen_mask_list, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel()\n",
    "best_checkpoint = torch.load('./transformer_best_model_0.7.pth')\n",
    "model.load_state_dict(best_checkpoint)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    pred_proba_label = []\n",
    "    with torch.no_grad():\n",
    "        for epitope_ids_list, epitope_mask_list, left_antigen_ids_list, left_antigen_mask_list, right_antigen_ids_list, right_antigen_mask_list in tqdm(iter(test_loader)):\n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "\n",
    "            left_antigen_ids_list = left_antigen_ids_list.to(device)\n",
    "            left_antigen_mask_list = left_antigen_mask_list.to(device)\n",
    "\n",
    "            right_antigen_ids_list = right_antigen_ids_list.to(device)\n",
    "            right_antigen_mask_list = right_antigen_mask_list.to(device)\n",
    "            \n",
    "            model_pred = model(epitope_ids_list, epitope_mask_list, left_antigen_ids_list, left_antigen_mask_list, right_antigen_ids_list, right_antigen_mask_list)\n",
    "            model_pred = torch.sigmoid(model_pred).to('cpu')\n",
    "            \n",
    "            pred_proba_label += model_pred.tolist()\n",
    "    \n",
    "    pred_label = np.where(np.array(pred_proba_label)>CFG['THRESHOLD'], 1, 0)\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca48423",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78270dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56756af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submission/submit3.csv', index=False)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e22188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
