{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b31e5170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForPreTraining\n",
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53337460",
   "metadata": {},
   "source": [
    "## 전체 코드 process\n",
    "    1. 데이터 전처리\n",
    "        - 결측치 확인\n",
    "        - outlier 제거\n",
    "    2. 데이터 처리 및 representation 구성\n",
    "        - Antigen: 4가지 protein feature (isoelectric_point, aromaticity, gravy, instability_index)\n",
    "        - epitope: transformer로 부터 학습된 [CLS] token의 embedding\n",
    "    3. 모델링\n",
    "        - 적절한 사이즈의 transforemr encoder를 쌓아 모델 구성\n",
    "        - Transformer로 부터 학습된 epitope의 embedding과 antigen의 4가지 protein feature를 합친 후 dense layer를 거쳐 classification\n",
    "        - Data의 imbalance가 크기 때문에 Focal loss를 loss function으로 사용\n",
    "        - 5 fold cross validation의 결과를 바탕으로 hard voting ensemble을 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41571857",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b763fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>epitope_seq</th>\n",
       "      <th>antigen_seq</th>\n",
       "      <th>antigen_code</th>\n",
       "      <th>start_position</th>\n",
       "      <th>end_position</th>\n",
       "      <th>number_of_tested</th>\n",
       "      <th>number_of_responses</th>\n",
       "      <th>assay_method_technique</th>\n",
       "      <th>assay_group</th>\n",
       "      <th>disease_type</th>\n",
       "      <th>disease_state</th>\n",
       "      <th>reference_date</th>\n",
       "      <th>reference_journal</th>\n",
       "      <th>reference_title</th>\n",
       "      <th>reference_IRI</th>\n",
       "      <th>qualitative_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200001</td>\n",
       "      <td>KGILSN</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>P02622.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antigen inhibition</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Occurrence of allergy</td>\n",
       "      <td>allergic disease</td>\n",
       "      <td>1976</td>\n",
       "      <td>Int Arch Allergy Appl Immunol</td>\n",
       "      <td>The allergenic structure of allergen M from co...</td>\n",
       "      <td>http://www.iedb.org/reference/1005599</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200002</td>\n",
       "      <td>SNADIK</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>P02622.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antigen inhibition</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Occurrence of allergy</td>\n",
       "      <td>allergic disease</td>\n",
       "      <td>1976</td>\n",
       "      <td>Int Arch Allergy Appl Immunol</td>\n",
       "      <td>The allergenic structure of allergen M from co...</td>\n",
       "      <td>http://www.iedb.org/reference/1005599</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200003</td>\n",
       "      <td>EGSFDEDGFYAKVGLDAFSADELK</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>P02622.1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antigen inhibition</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Occurrence of allergy</td>\n",
       "      <td>allergic disease</td>\n",
       "      <td>1976</td>\n",
       "      <td>Int Arch Allergy Appl Immunol</td>\n",
       "      <td>The allergenic structure of allergen M from co...</td>\n",
       "      <td>http://www.iedb.org/reference/1005599</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200004</td>\n",
       "      <td>SFDEDGFY</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>P02622.1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antigen inhibition</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Occurrence of allergy</td>\n",
       "      <td>allergic disease</td>\n",
       "      <td>1976</td>\n",
       "      <td>Int Arch Allergy Appl Immunol</td>\n",
       "      <td>The allergenic structure of allergen M from co...</td>\n",
       "      <td>http://www.iedb.org/reference/1005599</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200005</td>\n",
       "      <td>DEDGFY</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>P02622.1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>antigen inhibition</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Occurrence of allergy</td>\n",
       "      <td>allergic disease</td>\n",
       "      <td>1976</td>\n",
       "      <td>Int Arch Allergy Appl Immunol</td>\n",
       "      <td>The allergenic structure of allergen M from co...</td>\n",
       "      <td>http://www.iedb.org/reference/1005599</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id               epitope_seq  \\\n",
       "0  200001                    KGILSN   \n",
       "1  200002                    SNADIK   \n",
       "2  200003  EGSFDEDGFYAKVGLDAFSADELK   \n",
       "3  200004                  SFDEDGFY   \n",
       "4  200005                    DEDGFY   \n",
       "\n",
       "                                         antigen_seq antigen_code  \\\n",
       "0  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...     P02622.1   \n",
       "1  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...     P02622.1   \n",
       "2  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...     P02622.1   \n",
       "3  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...     P02622.1   \n",
       "4  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...     P02622.1   \n",
       "\n",
       "   start_position  end_position  number_of_tested  number_of_responses  \\\n",
       "0             3.0           8.0               NaN                  NaN   \n",
       "1             7.0          12.0               NaN                  NaN   \n",
       "2            21.0          44.0               NaN                  NaN   \n",
       "3            23.0          30.0               NaN                  NaN   \n",
       "4            25.0          30.0               NaN                  NaN   \n",
       "\n",
       "  assay_method_technique          assay_group           disease_type  \\\n",
       "0     antigen inhibition  qualitative binding  Occurrence of allergy   \n",
       "1     antigen inhibition  qualitative binding  Occurrence of allergy   \n",
       "2     antigen inhibition  qualitative binding  Occurrence of allergy   \n",
       "3     antigen inhibition  qualitative binding  Occurrence of allergy   \n",
       "4     antigen inhibition  qualitative binding  Occurrence of allergy   \n",
       "\n",
       "      disease_state  reference_date              reference_journal  \\\n",
       "0  allergic disease            1976  Int Arch Allergy Appl Immunol   \n",
       "1  allergic disease            1976  Int Arch Allergy Appl Immunol   \n",
       "2  allergic disease            1976  Int Arch Allergy Appl Immunol   \n",
       "3  allergic disease            1976  Int Arch Allergy Appl Immunol   \n",
       "4  allergic disease            1976  Int Arch Allergy Appl Immunol   \n",
       "\n",
       "                                     reference_title  \\\n",
       "0  The allergenic structure of allergen M from co...   \n",
       "1  The allergenic structure of allergen M from co...   \n",
       "2  The allergenic structure of allergen M from co...   \n",
       "3  The allergenic structure of allergen M from co...   \n",
       "4  The allergenic structure of allergen M from co...   \n",
       "\n",
       "                           reference_IRI qualitative_label  label  \n",
       "0  http://www.iedb.org/reference/1005599          Positive      1  \n",
       "1  http://www.iedb.org/reference/1005599          Positive      1  \n",
       "2  http://www.iedb.org/reference/1005599          Positive      1  \n",
       "3  http://www.iedb.org/reference/1005599          Positive      1  \n",
       "4  http://www.iedb.org/reference/1005599          Positive      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75c3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# antigen과 epitope 길이 가져오기\n",
    "train_df['antigen_length'] = train_df.antigen_seq.map(lambda a:len(a))\n",
    "train_df['epitope_length'] = train_df.epitope_seq.map(lambda a:len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99187e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 190811 entries, 0 to 190810\n",
      "Data columns (total 20 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   id                      190811 non-null  int64  \n",
      " 1   epitope_seq             190811 non-null  object \n",
      " 2   antigen_seq             190811 non-null  object \n",
      " 3   antigen_code            190811 non-null  object \n",
      " 4   start_position          190811 non-null  float64\n",
      " 5   end_position            190811 non-null  float64\n",
      " 6   number_of_tested        39198 non-null   float64\n",
      " 7   number_of_responses     38353 non-null   float64\n",
      " 8   assay_method_technique  190811 non-null  object \n",
      " 9   assay_group             190811 non-null  object \n",
      " 10  disease_type            190811 non-null  object \n",
      " 11  disease_state           180793 non-null  object \n",
      " 12  reference_date          190811 non-null  int64  \n",
      " 13  reference_journal       190811 non-null  object \n",
      " 14  reference_title         190811 non-null  object \n",
      " 15  reference_IRI           190811 non-null  object \n",
      " 16  qualitative_label       190811 non-null  object \n",
      " 17  label                   190811 non-null  int64  \n",
      " 18  antigen_length          190811 non-null  int64  \n",
      " 19  epitope_length          190811 non-null  int64  \n",
      "dtypes: float64(4), int64(5), object(11)\n",
      "memory usage: 29.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전체에 결측치 확인하기\n",
    "# epitope_seq, antigen_seq, antigen_length, epitope_length만 사용할 예정이기 때문에 다른 column에 결측치가 있어도 상관 없다.\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b39762c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epitope_seq</th>\n",
       "      <th>antigen_seq</th>\n",
       "      <th>antigen_length</th>\n",
       "      <th>epitope_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KGILSN</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>113</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SNADIK</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>113</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EGSFDEDGFYAKVGLDAFSADELK</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>113</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SFDEDGFY</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>113</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEDGFY</td>\n",
       "      <td>AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...</td>\n",
       "      <td>113</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                epitope_seq  \\\n",
       "0                    KGILSN   \n",
       "1                    SNADIK   \n",
       "2  EGSFDEDGFYAKVGLDAFSADELK   \n",
       "3                  SFDEDGFY   \n",
       "4                    DEDGFY   \n",
       "\n",
       "                                         antigen_seq  antigen_length  \\\n",
       "0  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...             113   \n",
       "1  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...             113   \n",
       "2  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...             113   \n",
       "3  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...             113   \n",
       "4  AFKGILSNADIKAAEAACFKEGSFDEDGFYAKVGLDAFSADELKKL...             113   \n",
       "\n",
       "   epitope_length  \n",
       "0               6  \n",
       "1               6  \n",
       "2              24  \n",
       "3               8  \n",
       "4               6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사용할 column만 남기기\n",
    "col = ['epitope_seq', 'antigen_seq', 'antigen_length', 'epitope_length']\n",
    "train_df = train_df[col]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d61fc45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antigen_length</th>\n",
       "      <th>epitope_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>190811.000000</td>\n",
       "      <td>190811.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>471.218006</td>\n",
       "      <td>14.462762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>423.717996</td>\n",
       "      <td>2.478726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>258.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>379.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>507.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4967.000000</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       antigen_length  epitope_length\n",
       "count   190811.000000   190811.000000\n",
       "mean       471.218006       14.462762\n",
       "std        423.717996        2.478726\n",
       "min         13.000000        2.000000\n",
       "25%        258.000000       15.000000\n",
       "50%        379.000000       15.000000\n",
       "75%        507.000000       15.000000\n",
       "max       4967.000000      250.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Antigen_length 분포 확인하기\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9acfa13b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEICAYAAABBBrPDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYZ0lEQVR4nO3df7CeZX3n8feniaCrIiApEwka1NQZZNoIqWa26rDSQqC1QYdq2K2kLkt0hN063e6KOrswKl3srjrDjNKBkjVY5YcgS7qGYsrSut0uyEGRX4ocAkwSA4n81MWCwHf/eK6jN8dz7nM45+QckvN+zTxz7ud7X9d1X5cPnI/3j/OQqkKSpPH8ylxPQJL0wmZQSJJ6GRSSpF4GhSSpl0EhSeplUEiSehkUmpeS3JHk6Lmex4gkZyf5qzk69t8l+TdzcWztGQwKzUtV9caq+juY21/Ss20+rVUzx6CQJPUyKLRHS/KqJFcm2ZXk3iT/rtXPTnJFksuS/DjJt5P8RqfffUl+O8kq4GPAe5P8JMl3O+NuTPJwkuEkp3X6TjT2mHN6nutameQfkzya5Lvdy2TtUtEnk/yfdvxvJDmos/+UJPcneSjJf5porc1rxhtPMii0x0ryK8BfA98FDgGOAT6c5LjWZDXwVeBA4CvA/0jyou4YVfU3wJ8Bl1XVy6pq5Bf+pcA24FXAScCfJXlHp+uYY09iTpNZ1yHA14FPtfH/FLgyyaJOs38JvB/4VWCf1oYkhwNfAP4VsBh4RZtH31rHHU8Cg0J7tt8EFlXVJ6rqqaraAlwIrGn7b66qK6rqZ8BngRcDKycaNMmhwG8BH6mqf6qqW4C/BE7pNBtv7InmNBl/CGyqqk1V9WxVbQaGgBM6bf57Vf2gqn4KXA4sb/WTgL+uqn+oqqeA/wxM5gvdxhtPYuFcT0CahtcAr0ryaKe2APjfwP3A1pFiVT2bZOQMYSKvAh6uqh93avcDKzrvxxu7euY0Wa8B/iDJOzu1FwHXd94/0Nl+AnhZZ+7duT2R5KFJHHO88SSDQnu0rcC9VbVs9I4kZwOHdt7/CrAE+OEY44z+f9w/BA5M8vJOWLwa2N5pM97YT483p+dhK/Clqjptwpa/bAfwhs7cXgK8srPfr4vW8+alJ+3JvgX8OMlHkrwkyYIkRyT5zbb/qCTvTrIQ+DDwJHDDGOM8CCxtv/Cpqq3APwL/JcmLk/w6cCrQfax0vLEnmtNk/BXwziTHtf4vTnJ0kiWT6HtF6/vPk+wDnA1kvLVKk+E/LNpjVdUzwO8xuJ5+L/AjBvcSXtGaXA28F3gEeB/w7nZPYbSvtp8PJfl22z4ZWMrgLOEq4Kyq+ttOnzHHnsScJrOurQxuln8M2MXgDOM/MIl/X6vqDuDfMrgZvwP4CbCTQZCNt1apV/wPF2lv1C49vb6q/nBPGnumJXkZ8CiwrKrunePpaA/lGYW0l0nyziT/LMlLgf8G3AbcN7ez0p7MoJBmSZJr2h+6jX59bIYPtZrBJbMfAsuANeWlA02Dl54kSb08o5Ak9drr/o7ioIMOqqVLl871NCRpj3LzzTf/qKoWjbVvrwuKpUuXMjQ0NNfTkKQ9SpL7x9vnpSdJUi+DQpLUy6CQJPUyKCRJvQwKSVIvg0KS1MugkCT1MigkSb0MCklSr73uL7OnY+mZX5+zY9937u/O2bElqY9nFJKkXgaFJKmXQSFJ6mVQSJJ6GRSSpF4GhSSpl0EhSeplUEiSehkUkqReBoUkqZdBIUnqZVBIknpNGBRJ1ifZmeT2Tu2yJLe0131Jbmn1pUl+2tn3F50+RyW5LclwkvOSpNUPTLI5yd3t5wGtntZuOMmtSY6c8dVLkiY0mTOKLwKruoWqem9VLa+q5cCVwNc6u+8Z2VdVH+zUzwdOA5a118iYZwLXVdUy4Lr2HuD4Ttt1rb8kaZZNGBRV9U3g4bH2tbOC9wCX9I2RZDGwX1XdUFUFXAyc2HavBja07Q2j6hfXwA3A/m0cSdIsmu49ircBD1bV3Z3aYUm+k+Tvk7yt1Q4BtnXabGs1gIOrakfbfgA4uNNn6zh9niPJuiRDSYZ27do1jeVIkkabblCczHPPJnYAr66qNwF/AnwlyX6THaydbdTznURVXVBVK6pqxaJFi55vd0lSjyn/F+6SLATeDRw1UquqJ4En2/bNSe4Bfg3YDizpdF/SagAPJllcVTvapaWdrb4dOHScPpKkWTKdM4rfBr5fVT+/pJRkUZIFbfu1DG5Eb2mXlh5PsrLd1zgFuLp12wisbdtrR9VPaU8/rQQe61yikiTNksk8HnsJ8H+BNyTZluTUtmsNv3wT++3Are1x2SuAD1bVyI3wDwF/CQwD9wDXtPq5wO8kuZtB+Jzb6puALa39ha2/JGmWTXjpqapOHqf+R2PUrmTwuOxY7YeAI8aoPwQcM0a9gNMnmp8kaffyL7MlSb0MCklSL4NCktTLoJAk9TIoJEm9DApJUi+DQpLUy6CQJPUyKCRJvQwKSVIvg0KS1MugkCT1MigkSb0MCklSL4NCktTLoJAk9TIoJEm9DApJUq/J/Dez1yfZmeT2Tu3sJNuT3NJeJ3T2fTTJcJK7khzXqa9qteEkZ3bqhyW5sdUvS7JPq+/b3g+3/UtnbNWSpEmbzBnFF4FVY9Q/V1XL22sTQJLDgTXAG1ufLyRZkGQB8HngeOBw4OTWFuDTbazXA48Ap7b6qcAjrf651k6SNMsmDIqq+ibw8CTHWw1cWlVPVtW9wDDw5vYarqotVfUUcCmwOkmAdwBXtP4bgBM7Y21o21cAx7T2kqRZNJ17FGckubVdmjqg1Q4BtnbabGu18eqvBB6tqqdH1Z8zVtv/WGv/S5KsSzKUZGjXrl3TWJIkabSpBsX5wOuA5cAO4DMzNaGpqKoLqmpFVa1YtGjRXE5FkvY6UwqKqnqwqp6pqmeBCxlcWgLYDhzaabqk1carPwTsn2ThqPpzxmr7X9HaS5Jm0ZSCIsniztt3ASNPRG0E1rQnlg4DlgHfAm4ClrUnnPZhcMN7Y1UVcD1wUuu/Fri6M9batn0S8L9ae0nSLFo4UYMklwBHAwcl2QacBRydZDlQwH3ABwCq6o4klwN3Ak8Dp1fVM22cM4BrgQXA+qq6ox3iI8ClST4FfAe4qNUvAr6UZJjBzfQ1012sJOn5mzAoqurkMcoXjVEbaX8OcM4Y9U3ApjHqW/jFpatu/Z+AP5hofpKk3cu/zJYk9TIoJEm9DApJUi+DQpLUy6CQJPUyKCRJvQwKSVIvg0KS1MugkCT1MigkSb0MCklSL4NCktTLoJAk9TIoJEm9DApJUi+DQpLUy6CQJPUyKCRJvSYMiiTrk+xMcnun9l+TfD/JrUmuSrJ/qy9N8tMkt7TXX3T6HJXktiTDSc5LklY/MMnmJHe3nwe0elq74XacI2d89ZKkCU3mjOKLwKpRtc3AEVX168APgI929t1TVcvb64Od+vnAacCy9hoZ80zguqpaBlzX3gMc32m7rvWXJM2yCYOiqr4JPDyq9o2qerq9vQFY0jdGksXAflV1Q1UVcDFwYtu9GtjQtjeMql9cAzcA+7dxJEmzaCbuUfxr4JrO+8OSfCfJ3yd5W6sdAmzrtNnWagAHV9WOtv0AcHCnz9Zx+jxHknVJhpIM7dq1axpLkSSNNq2gSPJx4Gngy620A3h1Vb0J+BPgK0n2m+x47Wyjnu88quqCqlpRVSsWLVr0fLtLknosnGrHJH8E/B5wTPsFT1U9CTzZtm9Ocg/wa8B2nnt5akmrATyYZHFV7WiXlna2+nbg0HH6SJJmyZTOKJKsAv4j8PtV9USnvijJgrb9WgY3ore0S0uPJ1nZnnY6Bbi6ddsIrG3ba0fVT2lPP60EHutcopIkzZIJzyiSXAIcDRyUZBtwFoOnnPYFNrenXG9oTzi9HfhEkp8BzwIfrKqRG+EfYvAE1UsY3NMYua9xLnB5klOB+4H3tPom4ARgGHgCeP90FipJmpoJg6KqTh6jfNE4ba8Erhxn3xBwxBj1h4BjxqgXcPpE85Mk7V7+ZbYkqZdBIUnqZVBIknoZFJKkXgaFJKmXQSFJ6mVQSJJ6GRSSpF4GhSSpl0EhSeplUEiSehkUkqReBoUkqZdBIUnqZVBIknoZFJKkXgaFJKmXQSFJ6mVQSJJ6TSookqxPsjPJ7Z3agUk2J7m7/Tyg1ZPkvCTDSW5NcmSnz9rW/u4kazv1o5Lc1vqclyR9x5AkzZ7JnlF8EVg1qnYmcF1VLQOua+8BjgeWtdc64HwY/NIHzgLeArwZOKvzi/984LROv1UTHEOSNEsmFRRV9U3g4VHl1cCGtr0BOLFTv7gGbgD2T7IYOA7YXFUPV9UjwGZgVdu3X1XdUFUFXDxqrLGOIUmaJdO5R3FwVe1o2w8AB7ftQ4CtnXbbWq2vvm2Met8xniPJuiRDSYZ27do1xeVIksYyIzez25lAzcRYUzlGVV1QVSuqasWiRYt25zQkad6ZTlA82C4b0X7ubPXtwKGddktara++ZIx63zEkSbNkOkGxERh5cmktcHWnfkp7+mkl8Fi7fHQtcGySA9pN7GOBa9u+x5OsbE87nTJqrLGOIUmaJQsn0yjJJcDRwEFJtjF4eulc4PIkpwL3A+9pzTcBJwDDwBPA+wGq6uEknwRuau0+UVUjN8g/xODJqpcA17QXPceQJM2SSQVFVZ08zq5jxmhbwOnjjLMeWD9GfQg4Yoz6Q2MdQ5I0e/zLbElSL4NCktTLoJAk9TIoJEm9DApJUi+DQpLUy6CQJPUyKCRJvQwKSVIvg0KS1MugkCT1MigkSb0MCklSL4NCktTLoJAk9TIoJEm9DApJUi+DQpLUa8pBkeQNSW7pvB5P8uEkZyfZ3qmf0Onz0STDSe5KclynvqrVhpOc2akfluTGVr8syT5TX6okaSqmHBRVdVdVLa+q5cBRwBPAVW3350b2VdUmgCSHA2uANwKrgC8kWZBkAfB54HjgcODk1hbg022s1wOPAKdOdb6SpKmZqUtPxwD3VNX9PW1WA5dW1ZNVdS8wDLy5vYaraktVPQVcCqxOEuAdwBWt/wbgxBmaryRpkmYqKNYAl3Ten5Hk1iTrkxzQaocAWztttrXaePVXAo9W1dOj6r8kybokQ0mGdu3aNf3VSJJ+btpB0e4b/D7w1VY6H3gdsBzYAXxmuseYSFVdUFUrqmrFokWLdvfhJGleWTgDYxwPfLuqHgQY+QmQ5ELgf7a324FDO/2WtBrj1B8C9k+ysJ1VdNtLkmbJTFx6OpnOZackizv73gXc3rY3AmuS7JvkMGAZ8C3gJmBZe8JpHwaXsTZWVQHXAye1/muBq2dgvpKk52FaZxRJXgr8DvCBTvnPkywHCrhvZF9V3ZHkcuBO4Gng9Kp6po1zBnAtsABYX1V3tLE+Alya5FPAd4CLpjNfSdLzN62gqKr/x+Cmc7f2vp725wDnjFHfBGwao76FwVNRkqQ54l9mS5J6GRSSpF4GhSSpl0EhSeplUEiSehkUkqReBoUkqZdBIUnqZVBIknoZFJKkXgaFJKmXQSFJ6mVQSJJ6GRSSpF4GhSSpl0EhSeplUEiSehkUkqRe0w6KJPcluS3JLUmGWu3AJJuT3N1+HtDqSXJekuEktyY5sjPO2tb+7iRrO/Wj2vjDrW+mO2dJ0uTN1BnFv6iq5VW1or0/E7iuqpYB17X3AMcDy9prHXA+DIIFOAt4C4P/RvZZI+HS2pzW6bdqhuYsSZqE3XXpaTWwoW1vAE7s1C+ugRuA/ZMsBo4DNlfVw1X1CLAZWNX27VdVN1RVARd3xpIkzYKZCIoCvpHk5iTrWu3gqtrRth8ADm7bhwBbO323tVpffdsY9edIsi7JUJKhXbt2TXc9kqSOhTMwxluranuSXwU2J/l+d2dVVZKageOMq6ouAC4AWLFixW49liTNN9M+o6iq7e3nTuAqBvcYHmyXjWg/d7bm24FDO92XtFpffckYdUnSLJlWUCR5aZKXj2wDxwK3AxuBkSeX1gJXt+2NwCnt6aeVwGPtEtW1wLFJDmg3sY8Frm37Hk+ysj3tdEpnLEnSLJjupaeDgavaE6sLga9U1d8kuQm4PMmpwP3Ae1r7TcAJwDDwBPB+gKp6OMkngZtau09U1cNt+0PAF4GXANe0lyRplkwrKKpqC/AbY9QfAo4Zo17A6eOMtR5YP0Z9CDhiOvOUJE2df5ktSeplUEiSehkUkqReBoUkqZdBIUnqZVBIknoZFJKkXgaFJKmXQSFJ6mVQSJJ6GRSSpF4GhSSpl0EhSeplUEiSehkUkqReBoUkqZdBIUnqZVBIknpNOSiSHJrk+iR3JrkjyR+3+tlJtie5pb1O6PT5aJLhJHclOa5TX9Vqw0nO7NQPS3Jjq1+WZJ+pzleSNDXTOaN4Gvj3VXU4sBI4Pcnhbd/nqmp5e20CaPvWAG8EVgFfSLIgyQLg88DxwOHAyZ1xPt3Gej3wCHDqNOYrSZqCKQdFVe2oqm+37R8D3wMO6emyGri0qp6sqnuBYeDN7TVcVVuq6ingUmB1kgDvAK5o/TcAJ051vpKkqZmRexRJlgJvAm5spTOS3JpkfZIDWu0QYGun27ZWG6/+SuDRqnp6VF2SNIumHRRJXgZcCXy4qh4HzgdeBywHdgCfme4xJjGHdUmGkgzt2rVrdx9OkuaVaQVFkhcxCIkvV9XXAKrqwap6pqqeBS5kcGkJYDtwaKf7klYbr/4QsH+ShaPqv6SqLqiqFVW1YtGiRdNZkiRplOk89RTgIuB7VfXZTn1xp9m7gNvb9kZgTZJ9kxwGLAO+BdwELGtPOO3D4Ib3xqoq4HrgpNZ/LXD1VOcrSZqahRM3GddvAe8DbktyS6t9jMFTS8uBAu4DPgBQVXckuRy4k8ETU6dX1TMASc4ArgUWAOur6o423keAS5N8CvgOg2CSJM2iKQdFVf0DkDF2berpcw5wzhj1TWP1q6ot/OLSlSRpDviX2ZKkXgaFJKmXQSFJ6mVQSJJ6GRSSpF4GhSSpl0EhSeplUEiSehkUkqReBoUkqZdBIUnqZVBIknoZFJKkXgaFJKmXQSFJ6mVQSJJ6GRSSpF4GhSSpl0EhSer1gg+KJKuS3JVkOMmZcz0fSZpvXtBBkWQB8HngeOBw4OQkh8/trCRpflk41xOYwJuB4araApDkUmA1cOeczmo3WHrm1+fkuPed+7tzclxJe44XelAcAmztvN8GvGV0oyTrgHXt7U+S3PU8j3MQ8KMpzXDPdRDwo3x6rqcx6+btZz3Xk5hl83HNML11v2a8HS/0oJiUqroAuGCq/ZMMVdWKGZzSC958XDPMz3W75vljd637BX2PAtgOHNp5v6TVJEmz5IUeFDcBy5IclmQfYA2wcY7nJEnzygv60lNVPZ3kDOBaYAGwvqru2A2HmvJlqz3YfFwzzM91u+b5Y7esO1W1O8aVJO0lXuiXniRJc8ygkCT1mtdBMZ++HiTJfUluS3JLkqFWOzDJ5iR3t58HzPU8pyPJ+iQ7k9zeqY25xgyc1z77W5McOXczn55x1n12ku3t874lyQmdfR9t674ryXFzM+vpSXJokuuT3JnkjiR/3Op77efds+bd/1lX1bx8Mbg5fg/wWmAf4LvA4XM9r9243vuAg0bV/hw4s22fCXx6ruc5zTW+HTgSuH2iNQInANcAAVYCN871/Gd43WcDfzpG28PbP+v7Aoe1fwcWzPUaprDmxcCRbfvlwA/a2vbaz7tnzbv9s57PZxQ//3qQqnoKGPl6kPlkNbChbW8ATpy7qUxfVX0TeHhUebw1rgYuroEbgP2TLJ6Vic6wcdY9ntXApVX1ZFXdCwwz+Hdhj1JVO6rq2237x8D3GHyTw177efeseTwz9lnP56AY6+tB+v5H39MV8I0kN7evPAE4uKp2tO0HgIPnZmq71XhrnA+f/xntMsv6zmXFvW7dSZYCbwJuZJ583qPWDLv5s57PQTHfvLWqjmTwTbynJ3l7d2cNzlX36mel58MaO84HXgcsB3YAn5nT2ewmSV4GXAl8uKoe7+7bWz/vMda82z/r+RwU8+rrQapqe/u5E7iKwSnogyOn3+3nzrmb4W4z3hr36s+/qh6sqmeq6lngQn5xyWGvWXeSFzH4hfnlqvpaK+/Vn/dYa56Nz3o+B8W8+XqQJC9N8vKRbeBY4HYG613bmq0Frp6bGe5W461xI3BKexpmJfBY55LFHm/U9fd3Mfi8YbDuNUn2TXIYsAz41mzPb7qSBLgI+F5Vfbaza6/9vMdb86x81nN9J38uXwyehPgBg6cBPj7X89mN63wtg6cfvgvcMbJW4JXAdcDdwN8CB871XKe5zksYnHr/jMH12FPHWyODp18+3z7724AVcz3/GV73l9q6bm2/MBZ32n+8rfsu4Pi5nv8U1/xWBpeVbgVuaa8T9ubPu2fNu/2z9is8JEm95vOlJ0nSJBgUkqReBoUkqZdBIUnqZVBIknoZFJKkXgaFJKnX/wfv6vssNKdUAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_df['antigen_length'])\n",
    "plt.title('antigen_length')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(train_df['epitope_length'])\n",
    "plt.title('epitope_length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27eaf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epitope_seq</th>\n",
       "      <th>antigen_seq</th>\n",
       "      <th>antigen_length</th>\n",
       "      <th>epitope_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>LAQTHSAIAVIIGIKDLDAFRHYDGRTIIQRDNGYQPNYHAVNIVG...</td>\n",
       "      <td>RPSSIKTFEEYKKAFNKSYATFEDEEAARKNFLESVKYVQSNGGAI...</td>\n",
       "      <td>302</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>QSCRRPNAQRFGISNYCQIYPPNANKIREALAQTHSAIAVIIGIKD...</td>\n",
       "      <td>RPSSIKTFEEYKKAFNKSYATFEDEEAARKNFLESVKYVQSNGGAI...</td>\n",
       "      <td>302</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>QELVDCASQHGCHGDTIPRGIEYIQHNGVVQESYYRYVAREQSCRR...</td>\n",
       "      <td>MKIVLAIASLLALSAVYARPSSIKTFEEYKKAFNKSYATFEDEEAA...</td>\n",
       "      <td>320</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>TNACSINGNAPAEIDLRQMRTVTPIRMQGGCGSCWAFSGVAATESA...</td>\n",
       "      <td>MSAEAFEHLKTQFDLNAETNACSINGNAPAEIDLRQMRTVTPIRMQ...</td>\n",
       "      <td>96</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>WAQSTDFPQFKPEEITAIMNDFNEPGSLAPTGLYLGGTKYMVIQGE...</td>\n",
       "      <td>MSWQAYVDDHLLCDIEGNHLTHAAIIGQDGSVWAQSTDFPQFKPEE...</td>\n",
       "      <td>131</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7511</th>\n",
       "      <td>ETTARIIFNGKDLNLVERRIAAVNPSDPLETTKPDMTLKEALKIAF...</td>\n",
       "      <td>MKKRKVLIPLMALSTILVSSTGNLEVIQAEVKQENRLLNESESSSQ...</td>\n",
       "      <td>764</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11502</th>\n",
       "      <td>TAGEGIVIYGVDTGIDIGHADFGGRAEWGTNTADNDDTDGNGHGTH...</td>\n",
       "      <td>MGFLKLLSTSLATLAVVNAGKLLTANDGDEVVPSSYIVVMNDGVST...</td>\n",
       "      <td>398</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11504</th>\n",
       "      <td>FGGRAEWGTNTADNDDTDGNGHGTHTASTAAGSKFGVAKKASVVAV...</td>\n",
       "      <td>MGFLKLLSTSLATLAVVNAGKLLTANDGDEVVPSSYIVVMNDGVST...</td>\n",
       "      <td>398</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11598</th>\n",
       "      <td>EGLRMMMRMMQQKEMQPRGEQMRRMMRLAENIPSRCNLSPMRCPMG...</td>\n",
       "      <td>MAKISVAAAALLVLMALGHATAFRATVTTTVVEEENQEECREQMQR...</td>\n",
       "      <td>146</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13228</th>\n",
       "      <td>CVCRSCRTMSLQKTVEKLFDELDKDKSGKISCAELKSALQSCSAEP...</td>\n",
       "      <td>CVCRSCRTMSLQKTVEKLFDELDKDKSGKISCAELKSALQSCSAEP...</td>\n",
       "      <td>76</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13245</th>\n",
       "      <td>EIINEIEKKIEDIEKNINITKENLKELENKITELQSSFSSYENEMK...</td>\n",
       "      <td>MYIEEIILDGFKSYPTKTVIGPFHPQFNAITGLNGSGKSNVLDAIC...</td>\n",
       "      <td>1218</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14742</th>\n",
       "      <td>DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIATVIV...</td>\n",
       "      <td>MLPGLALLLLAAWTARALEVPTDGNAGLLAEPQIAMFCGRLNMHMN...</td>\n",
       "      <td>770</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15430</th>\n",
       "      <td>MSLLTEVETYVLSIIPSGPLKAEIAQKLEDVFAGKNTDLEALMEWL...</td>\n",
       "      <td>MSLLTEVETYVLSIIPSGPLKAEIAQKLEDVFAGKNTDLEALMEWL...</td>\n",
       "      <td>252</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16567</th>\n",
       "      <td>EGKSVFDDNVQRGQILVVPQGFAVVVKAGRQGLEWVELKNNDNAIT...</td>\n",
       "      <td>GLEQAFCNLKFRQNVNRPSHADVFNPRAGRINTVNSNNLPILEFLQ...</td>\n",
       "      <td>195</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17582</th>\n",
       "      <td>LVERDHKNEFCEITLISSGRKDCNEIPTEGWAKPSLKFKLNTVNGT...</td>\n",
       "      <td>EDIPQPPVSQFHIQGQVYCDTCRAGFITELSEFIPGASLRLQCKDK...</td>\n",
       "      <td>145</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22790</th>\n",
       "      <td>MEGPFFRDYALNVFVGKVETNQLDLVASFVKNQTQHLMGNSLKDEP...</td>\n",
       "      <td>MPLALTLLLLSGLGAPGGWGCLQCDPLVLEALGHLRSALIPSRFQL...</td>\n",
       "      <td>221</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22793</th>\n",
       "      <td>LLKEEVLDCLHCQRITPKCIHKKYCFVDRQPRVALQYQMDSKYPRN...</td>\n",
       "      <td>MPLALTLLLLSGLGAPGGWGCLQCDPLVLEALGHLRSALIPSRFQL...</td>\n",
       "      <td>221</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             epitope_seq  \\\n",
       "781    LAQTHSAIAVIIGIKDLDAFRHYDGRTIIQRDNGYQPNYHAVNIVG...   \n",
       "782    QSCRRPNAQRFGISNYCQIYPPNANKIREALAQTHSAIAVIIGIKD...   \n",
       "783    QELVDCASQHGCHGDTIPRGIEYIQHNGVVQESYYRYVAREQSCRR...   \n",
       "795    TNACSINGNAPAEIDLRQMRTVTPIRMQGGCGSCWAFSGVAATESA...   \n",
       "7387   WAQSTDFPQFKPEEITAIMNDFNEPGSLAPTGLYLGGTKYMVIQGE...   \n",
       "7511   ETTARIIFNGKDLNLVERRIAAVNPSDPLETTKPDMTLKEALKIAF...   \n",
       "11502  TAGEGIVIYGVDTGIDIGHADFGGRAEWGTNTADNDDTDGNGHGTH...   \n",
       "11504  FGGRAEWGTNTADNDDTDGNGHGTHTASTAAGSKFGVAKKASVVAV...   \n",
       "11598  EGLRMMMRMMQQKEMQPRGEQMRRMMRLAENIPSRCNLSPMRCPMG...   \n",
       "13228  CVCRSCRTMSLQKTVEKLFDELDKDKSGKISCAELKSALQSCSAEP...   \n",
       "13245  EIINEIEKKIEDIEKNINITKENLKELENKITELQSSFSSYENEMK...   \n",
       "14742  DAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIATVIV...   \n",
       "15430  MSLLTEVETYVLSIIPSGPLKAEIAQKLEDVFAGKNTDLEALMEWL...   \n",
       "16567  EGKSVFDDNVQRGQILVVPQGFAVVVKAGRQGLEWVELKNNDNAIT...   \n",
       "17582  LVERDHKNEFCEITLISSGRKDCNEIPTEGWAKPSLKFKLNTVNGT...   \n",
       "22790  MEGPFFRDYALNVFVGKVETNQLDLVASFVKNQTQHLMGNSLKDEP...   \n",
       "22793  LLKEEVLDCLHCQRITPKCIHKKYCFVDRQPRVALQYQMDSKYPRN...   \n",
       "\n",
       "                                             antigen_seq  antigen_length  \\\n",
       "781    RPSSIKTFEEYKKAFNKSYATFEDEEAARKNFLESVKYVQSNGGAI...             302   \n",
       "782    RPSSIKTFEEYKKAFNKSYATFEDEEAARKNFLESVKYVQSNGGAI...             302   \n",
       "783    MKIVLAIASLLALSAVYARPSSIKTFEEYKKAFNKSYATFEDEEAA...             320   \n",
       "795    MSAEAFEHLKTQFDLNAETNACSINGNAPAEIDLRQMRTVTPIRMQ...              96   \n",
       "7387   MSWQAYVDDHLLCDIEGNHLTHAAIIGQDGSVWAQSTDFPQFKPEE...             131   \n",
       "7511   MKKRKVLIPLMALSTILVSSTGNLEVIQAEVKQENRLLNESESSSQ...             764   \n",
       "11502  MGFLKLLSTSLATLAVVNAGKLLTANDGDEVVPSSYIVVMNDGVST...             398   \n",
       "11504  MGFLKLLSTSLATLAVVNAGKLLTANDGDEVVPSSYIVVMNDGVST...             398   \n",
       "11598  MAKISVAAAALLVLMALGHATAFRATVTTTVVEEENQEECREQMQR...             146   \n",
       "13228  CVCRSCRTMSLQKTVEKLFDELDKDKSGKISCAELKSALQSCSAEP...              76   \n",
       "13245  MYIEEIILDGFKSYPTKTVIGPFHPQFNAITGLNGSGKSNVLDAIC...            1218   \n",
       "14742  MLPGLALLLLAAWTARALEVPTDGNAGLLAEPQIAMFCGRLNMHMN...             770   \n",
       "15430  MSLLTEVETYVLSIIPSGPLKAEIAQKLEDVFAGKNTDLEALMEWL...             252   \n",
       "16567  GLEQAFCNLKFRQNVNRPSHADVFNPRAGRINTVNSNNLPILEFLQ...             195   \n",
       "17582  EDIPQPPVSQFHIQGQVYCDTCRAGFITELSEFIPGASLRLQCKDK...             145   \n",
       "22790  MPLALTLLLLSGLGAPGGWGCLQCDPLVLEALGHLRSALIPSRFQL...             221   \n",
       "22793  MPLALTLLLLSGLGAPGGWGCLQCDPLVLEALGHLRSALIPSRFQL...             221   \n",
       "\n",
       "       epitope_length  \n",
       "781                57  \n",
       "782                54  \n",
       "783                52  \n",
       "795                56  \n",
       "7387               56  \n",
       "7511              250  \n",
       "11502              52  \n",
       "11504              51  \n",
       "11598              52  \n",
       "13228              51  \n",
       "13245              57  \n",
       "14742              55  \n",
       "15430              51  \n",
       "16567              51  \n",
       "17582              53  \n",
       "22790              51  \n",
       "22793              51  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 문제에서 epitope부분이 중요하고 대부분이 50 이하의 길이를 가진다. 따라서 50이상의 길이를 가진 eptiope을 확인\n",
    "train_df[train_df['epitope_length'] > 50]\n",
    "# Max length는 250 이지만 나머지 모든 epitope는 60이하의 길이를 가지는 것을 확인(7511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f6302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250의 길이를 가진 epitope_seq는 제거 (outlier)\n",
    "train_df = train_df.drop([7511])\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf7db8",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335a7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'NUM_WORKERS':4,\n",
    "    'EPITOPE_MAX_LEN':62,\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':5e-5,\n",
    "    'BATCH_SIZE':1024,\n",
    "    'THRESHOLD':0.5,   # 기본적으로 0.5로 사용하지만 data impalance가 심할 경우 더 큰 값을 사용하기도 한다.\n",
    "    'SEED':41\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:4') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cca36b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d50ba1",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "050f2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface protein tokenizer 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert_bfd', do_lower_case=False)\n",
    "\n",
    "def seqtoinput(seq):\n",
    "    for j in range(len(seq)-1):\n",
    "        seq = seq[:j+j+1]+ ' ' + seq[j+j+1:]\n",
    "    return seq \n",
    "\n",
    "# Antigen은 너무 길이가 길기 때문에 LM로 embedding을 얻는 것보다 protein feature를 얻는 것이 더 좋을 것이라고 생각해 아래의 4가지 feature를 사용\n",
    "def get_protein_feature(seq):\n",
    "    protein_feature = []\n",
    "    protein_feature.append(ProteinAnalysis(seq).isoelectric_point())\n",
    "    protein_feature.append(ProteinAnalysis(seq).aromaticity())\n",
    "    protein_feature.append(ProteinAnalysis(seq).gravy())\n",
    "    protein_feature.append(ProteinAnalysis(seq).instability_index())\n",
    "    return protein_feature\n",
    "\n",
    "def get_preprocessing(data_type, new_df, tokenizer):\n",
    "    epitope_ids_list = []\n",
    "    epitope_mask_list = []\n",
    "    \n",
    "    protein_features = []\n",
    "#     epitope_features = []\n",
    "        \n",
    "    for epitope, antigen, s_p, e_p in tqdm(zip(new_df['epitope_seq'], new_df['antigen_seq'], new_df['start_position'], new_df['end_position'])):             \n",
    "        protein_features.append(get_protein_feature(antigen))\n",
    "#         epitope_features.append(get_peptide_feature(epitope))\n",
    "        \n",
    "        epitope = seqtoinput(epitope)\n",
    "        \n",
    "        \n",
    "        epitope_input = tokenizer(epitope, add_special_tokens=True, pad_to_max_length=True, max_length = 62)\n",
    "        epitope_ids = epitope_input['input_ids']\n",
    "        epitope_mask = epitope_input['attention_mask']\n",
    "        \n",
    "        \n",
    "        epitope_ids_list.append(epitope_ids)\n",
    "        epitope_mask_list.append(epitope_mask)\n",
    "\n",
    "    \n",
    "    label_list = None\n",
    "    if data_type != 'test':\n",
    "        label_list = []\n",
    "        for label in new_df['label']:\n",
    "            label_list.append(label)\n",
    "    print(f'{data_type} dataframe preprocessing was done.')\n",
    "    \n",
    "    \n",
    "    return epitope_ids_list, epitope_mask_list, protein_features, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6c89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, epitope_ids_list, epitope_mask_list, protein_features, label_list):\n",
    "        self.epitope_ids_list = epitope_ids_list\n",
    "        self.epitope_mask_list = epitope_mask_list\n",
    "        self.protein_features = protein_features\n",
    "\n",
    "        self.label_list = label_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        self.epitope_ids = self.epitope_ids_list[index]\n",
    "        self.epitope_mask = self.epitope_mask_list[index]\n",
    "        \n",
    "        self.protein_feature = self.protein_features[index]        \n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            self.label = self.label_list[index]\n",
    "            return torch.tensor(self.epitope_ids), torch.tensor(self.epitope_mask), torch.tensor(self.protein_feature, dtype=torch.float32), self.label\n",
    "        else:\n",
    "            return torch.tensor(self.epitope_ids), torch.tensor(self.epitope_mask), torch.tensor(self.protein_feature, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.epitope_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1a1871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface를 활용해 transformer encoder layer가 12개 쌓인 모델 구성하기\n",
    "config = BertConfig(\n",
    "    vocab_size=30,\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    max_position_embeddings=62,\n",
    "    type_vocab_size=2, \n",
    ")\n",
    "\n",
    "pre = BertForPreTraining(config=config)\n",
    "pre.save_pretrained('model/bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4513e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6bc8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 protein_hidden_dim=768,\n",
    "                 pretrained_model='model/bert'\n",
    "                ):\n",
    "        super(TransformerModel, self).__init__()              \n",
    "        # Transformer                \n",
    "        self.transformer = BertModel.from_pretrained(pretrained_model)\n",
    "                \n",
    "        in_channels = protein_hidden_dim + 4\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.Linear(in_channels, protein_hidden_dim//4),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.BatchNorm1d(protein_hidden_dim//4),\n",
    "            nn.Linear(protein_hidden_dim//4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, epitope_ids_list, epitope_mask_list, protein_features):\n",
    "        # Get Embedding Vector\n",
    "        epitope_x = self.transformer(input_ids=epitope_ids_list, attention_mask=epitope_mask_list)[0]\n",
    "        \n",
    "        # transformer [CLS]\n",
    "        epitope_hidden = epitope_x[:, 0, :]\n",
    "                        \n",
    "        # Feature Concat -> Binary Classifier\n",
    "        x = torch.cat([epitope_hidden, protein_features], axis=-1)\n",
    "        x = self.classifier(x).view(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ce588c",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63eb7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=2):\n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha]).to(device)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d8b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device, split):\n",
    "    model.to(device)\n",
    "    criterion = WeightedFocalLoss().to(device) \n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for epitope_ids_list, epitope_mask_list, protein_features, label in tqdm(iter(train_loader)):\n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "\n",
    "            protein_features = protein_features.to(device)\n",
    "\n",
    "            label = label.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(epitope_ids_list, epitope_mask_list, protein_features)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                    \n",
    "        val_loss, val_f1, val_acc, val_f1_T, val_f1_F = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] Val F1 : [{val_f1:.5f}] Val acc : [{val_acc:.5f}] Val F1_T : [{val_f1_T:.5f}] Val F1_F : [{val_f1_F:.5f}] ')\n",
    "        \n",
    "        if best_val_f1 < val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "#             torch.save(model.module.state_dict(), 'epitope_best_model_{split}.pth', _use_new_zipfile_serialization=False)\n",
    "            # multi GPU를 사용했기 때문에 model.module로 모델 저장\n",
    "            torch.save(model.module.state_dict(), f'epitope_best_model_{split}.pth', _use_new_zipfile_serialization=False)\n",
    "            print('Model Saved.')\n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21d957f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    pred_proba_label = []\n",
    "    true_label = []\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for epitope_ids_list, epitope_mask_list, protein_features, label in tqdm(iter(val_loader)):\n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "\n",
    "            protein_features = protein_features.to(device)\n",
    "            \n",
    "            label = label.float().to(device)\n",
    "            \n",
    "            model_pred = model(epitope_ids_list, epitope_mask_list, protein_features)\n",
    "            loss = criterion(model_pred, label)\n",
    "            model_pred = torch.sigmoid(model_pred).to('cpu')\n",
    "            \n",
    "            pred_proba_label += model_pred.tolist()\n",
    "            true_label += label.to('cpu').tolist()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    pred_label = np.where(np.array(pred_proba_label)>CFG['THRESHOLD'], 1, 0)\n",
    "    val_f1 = f1_score(true_label, pred_label, average='macro')\n",
    "    f1_T = f1_score(true_label, pred_label)\n",
    "    f1_F = f1_score(true_label, pred_label, pos_label=0)\n",
    "    acc = accuracy_score(true_label, pred_label)\n",
    "    return np.mean(val_loss), val_f1, acc, f1_T, f1_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef6693",
   "metadata": {},
   "source": [
    "# test data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c38ed0ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "120944it [19:35, 102.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_epitope_ids_list, test_epitope_mask_list, test_protein_features, test_label_list= get_preprocessing('test', test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33011f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    pred_proba_label = []\n",
    "    with torch.no_grad():\n",
    "        for epitope_ids_list, epitope_mask_list, protein_features in tqdm(iter(test_loader)):\n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "\n",
    "            protein_features = protein_features.to(device)\n",
    "            \n",
    "            model_pred = model(epitope_ids_list, epitope_mask_list, protein_features)\n",
    "            model_pred = torch.sigmoid(model_pred).to('cpu')\n",
    "            \n",
    "            pred_proba_label += model_pred.tolist()\n",
    "    \n",
    "    pred_label = np.where(np.array(pred_proba_label)>CFG['THRESHOLD'], 1, 0)\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf28e20",
   "metadata": {},
   "source": [
    "# 5 fold cv + prediction\n",
    "시간 문제로 효율적인 코드를 작성하지는 못했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4687b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "152648it [03:57, 642.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38162it [00:58, 647.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 150/150 [03:26<00:00,  1.38s/it]\n",
      "100%|██████████| 38/38 [00:31<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [0.04681] Val Loss : [0.03734] Val F1 : [0.69575] Val acc : [0.89183] Val F1_T : [0.45150] Val F1_F : [0.94000] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [05:45<00:00,  2.30s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [0.04162] Val Loss : [0.03848] Val F1 : [0.66876] Val acc : [0.85446] Val F1_T : [0.42073] Val F1_F : [0.91678] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:46<00:00,  2.71s/it]\n",
      "100%|██████████| 38/38 [00:43<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [0.03897] Val Loss : [0.03517] Val F1 : [0.69870] Val acc : [0.88774] Val F1_T : [0.46005] Val F1_F : [0.93736] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [07:04<00:00,  2.83s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train Loss : [0.03677] Val Loss : [0.03468] Val F1 : [0.71059] Val acc : [0.90564] Val F1_T : [0.47300] Val F1_F : [0.94818] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [07:05<00:00,  2.83s/it]\n",
      "100%|██████████| 38/38 [00:41<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train Loss : [0.03487] Val Loss : [0.03421] Val F1 : [0.70635] Val acc : [0.89490] Val F1_T : [0.47105] Val F1_F : [0.94165] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:58<00:00,  2.79s/it]\n",
      "100%|██████████| 38/38 [00:44<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Train Loss : [0.03313] Val Loss : [0.03249] Val F1 : [0.71466] Val acc : [0.89830] Val F1_T : [0.48576] Val F1_F : [0.94357] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:58<00:00,  2.79s/it]\n",
      "100%|██████████| 38/38 [00:44<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Train Loss : [0.03155] Val Loss : [0.03167] Val F1 : [0.70725] Val acc : [0.89718] Val F1_T : [0.47144] Val F1_F : [0.94305] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [07:01<00:00,  2.81s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Train Loss : [0.03031] Val Loss : [0.03197] Val F1 : [0.71508] Val acc : [0.89429] Val F1_T : [0.48911] Val F1_F : [0.94105] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:52<00:00,  2.75s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Train Loss : [0.02907] Val Loss : [0.02982] Val F1 : [0.72267] Val acc : [0.91410] Val F1_T : [0.49226] Val F1_F : [0.95308] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:43<00:00,  2.69s/it]\n",
      "100%|██████████| 38/38 [00:41<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Train Loss : [0.02815] Val Loss : [0.02954] Val F1 : [0.73042] Val acc : [0.90629] Val F1_T : [0.51267] Val F1_F : [0.94816] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:45<00:00,  2.71s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [11] Train Loss : [0.02752] Val Loss : [0.02964] Val F1 : [0.71207] Val acc : [0.91654] Val F1_T : [0.46943] Val F1_F : [0.95471] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:41<00:00,  2.67s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [12] Train Loss : [0.02660] Val Loss : [0.02890] Val F1 : [0.72340] Val acc : [0.90608] Val F1_T : [0.49860] Val F1_F : [0.94819] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:45<00:00,  2.70s/it]\n",
      "100%|██████████| 38/38 [00:47<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [13] Train Loss : [0.02573] Val Loss : [0.02884] Val F1 : [0.72996] Val acc : [0.90674] Val F1_T : [0.51146] Val F1_F : [0.94845] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:29<00:00,  2.60s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [14] Train Loss : [0.02484] Val Loss : [0.02901] Val F1 : [0.72182] Val acc : [0.89820] Val F1_T : [0.50032] Val F1_F : [0.94333] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:28<00:00,  2.59s/it]\n",
      "100%|██████████| 38/38 [00:43<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [15] Train Loss : [0.02400] Val Loss : [0.02970] Val F1 : [0.72081] Val acc : [0.89204] Val F1_T : [0.50217] Val F1_F : [0.93945] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:30<00:00,  2.61s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [16] Train Loss : [0.02298] Val Loss : [0.02990] Val F1 : [0.71176] Val acc : [0.89526] Val F1_T : [0.48178] Val F1_F : [0.94174] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:33<00:00,  2.62s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [17] Train Loss : [0.02162] Val Loss : [0.03031] Val F1 : [0.73036] Val acc : [0.91080] Val F1_T : [0.50979] Val F1_F : [0.95094] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:31<00:00,  2.61s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [18] Train Loss : [0.02022] Val Loss : [0.03710] Val F1 : [0.67410] Val acc : [0.84388] Val F1_T : [0.43888] Val F1_F : [0.90932] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:29<00:00,  2.60s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [19] Train Loss : [0.01887] Val Loss : [0.03337] Val F1 : [0.71573] Val acc : [0.90273] Val F1_T : [0.48516] Val F1_F : [0.94629] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:36<00:00,  2.64s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [20] Train Loss : [0.01723] Val Loss : [0.03242] Val F1 : [0.70150] Val acc : [0.88821] Val F1_T : [0.46541] Val F1_F : [0.93758] \n",
      "Best Validation F1 Score : [0.73042]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 119/119 [03:30<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "152648it [03:57, 642.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38162it [00:59, 645.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 150/150 [03:38<00:00,  1.46s/it]\n",
      "100%|██████████| 38/38 [00:33<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [0.04898] Val Loss : [0.04166] Val F1 : [0.66819] Val acc : [0.85884] Val F1_T : [0.41668] Val F1_F : [0.91970] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:01<00:00,  2.41s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [0.04375] Val Loss : [0.03990] Val F1 : [0.69119] Val acc : [0.87797] Val F1_T : [0.45102] Val F1_F : [0.93135] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:38<00:00,  2.66s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [0.04086] Val Loss : [0.03644] Val F1 : [0.69122] Val acc : [0.88211] Val F1_T : [0.44845] Val F1_F : [0.93400] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:37<00:00,  2.65s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train Loss : [0.03836] Val Loss : [0.03762] Val F1 : [0.68052] Val acc : [0.86130] Val F1_T : [0.44019] Val F1_F : [0.92085] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:39<00:00,  2.66s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train Loss : [0.03613] Val Loss : [0.03404] Val F1 : [0.69764] Val acc : [0.88213] Val F1_T : [0.46145] Val F1_F : [0.93383] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:45<00:00,  2.70s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Train Loss : [0.03411] Val Loss : [0.03231] Val F1 : [0.71119] Val acc : [0.89867] Val F1_T : [0.47849] Val F1_F : [0.94388] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:40<00:00,  2.67s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Train Loss : [0.03247] Val Loss : [0.03407] Val F1 : [0.68672] Val acc : [0.86712] Val F1_T : [0.44898] Val F1_F : [0.92445] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:38<00:00,  2.65s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Train Loss : [0.03103] Val Loss : [0.03118] Val F1 : [0.71524] Val acc : [0.91078] Val F1_T : [0.47928] Val F1_F : [0.95121] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:36<00:00,  2.65s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Train Loss : [0.02951] Val Loss : [0.03007] Val F1 : [0.71406] Val acc : [0.91644] Val F1_T : [0.47350] Val F1_F : [0.95462] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:39<00:00,  2.66s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Train Loss : [0.02834] Val Loss : [0.02970] Val F1 : [0.72171] Val acc : [0.90910] Val F1_T : [0.49335] Val F1_F : [0.95007] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:32<00:00,  2.62s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [11] Train Loss : [0.02750] Val Loss : [0.03149] Val F1 : [0.68126] Val acc : [0.85611] Val F1_T : [0.44519] Val F1_F : [0.91734] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:45<00:00,  2.70s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [12] Train Loss : [0.02668] Val Loss : [0.02934] Val F1 : [0.72109] Val acc : [0.90970] Val F1_T : [0.49174] Val F1_F : [0.95045] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:50<00:00,  2.74s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [13] Train Loss : [0.02585] Val Loss : [0.02876] Val F1 : [0.72507] Val acc : [0.90886] Val F1_T : [0.50029] Val F1_F : [0.94986] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:41<00:00,  2.68s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [14] Train Loss : [0.02516] Val Loss : [0.02887] Val F1 : [0.71999] Val acc : [0.90090] Val F1_T : [0.49493] Val F1_F : [0.94506] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:36<00:00,  2.64s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [15] Train Loss : [0.02426] Val Loss : [0.02986] Val F1 : [0.71737] Val acc : [0.89786] Val F1_T : [0.49152] Val F1_F : [0.94323] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:44<00:00,  2.70s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [16] Train Loss : [0.02309] Val Loss : [0.02933] Val F1 : [0.71461] Val acc : [0.90365] Val F1_T : [0.48233] Val F1_F : [0.94688] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:35<00:00,  2.64s/it]\n",
      "100%|██████████| 38/38 [00:41<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [17] Train Loss : [0.02205] Val Loss : [0.02985] Val F1 : [0.72399] Val acc : [0.90847] Val F1_T : [0.49835] Val F1_F : [0.94964] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:39<00:00,  2.67s/it]\n",
      "100%|██████████| 38/38 [00:41<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [18] Train Loss : [0.02100] Val Loss : [0.03251] Val F1 : [0.68339] Val acc : [0.87042] Val F1_T : [0.44004] Val F1_F : [0.92673] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:37<00:00,  2.65s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [19] Train Loss : [0.01936] Val Loss : [0.03091] Val F1 : [0.70601] Val acc : [0.89814] Val F1_T : [0.46834] Val F1_F : [0.94368] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:42<00:00,  2.69s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [20] Train Loss : [0.01761] Val Loss : [0.03514] Val F1 : [0.69233] Val acc : [0.88342] Val F1_T : [0.44986] Val F1_F : [0.93480] \n",
      "Best Validation F1 Score : [0.72507]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 119/119 [03:36<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "152648it [03:57, 642.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38162it [00:59, 640.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 150/150 [03:40<00:00,  1.47s/it]\n",
      "100%|██████████| 38/38 [00:33<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [0.04725] Val Loss : [0.03715] Val F1 : [0.68385] Val acc : [0.88318] Val F1_T : [0.43282] Val F1_F : [0.93489] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:05<00:00,  2.43s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [0.04233] Val Loss : [0.03668] Val F1 : [0.69892] Val acc : [0.88633] Val F1_T : [0.46139] Val F1_F : [0.93646] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:30<00:00,  2.61s/it]\n",
      "100%|██████████| 38/38 [00:41<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [0.03976] Val Loss : [0.03740] Val F1 : [0.68480] Val acc : [0.86513] Val F1_T : [0.44638] Val F1_F : [0.92321] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:40<00:00,  2.67s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train Loss : [0.03752] Val Loss : [0.03434] Val F1 : [0.70160] Val acc : [0.89269] Val F1_T : [0.46281] Val F1_F : [0.94039] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:32<00:00,  2.62s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train Loss : [0.03556] Val Loss : [0.03635] Val F1 : [0.68926] Val acc : [0.86940] Val F1_T : [0.45267] Val F1_F : [0.92585] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:39<00:00,  2.66s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Train Loss : [0.03369] Val Loss : [0.03283] Val F1 : [0.71303] Val acc : [0.89602] Val F1_T : [0.48387] Val F1_F : [0.94219] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:40<00:00,  2.67s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Train Loss : [0.03212] Val Loss : [0.03306] Val F1 : [0.69073] Val acc : [0.86636] Val F1_T : [0.45768] Val F1_F : [0.92379] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:42<00:00,  2.68s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Train Loss : [0.03068] Val Loss : [0.03108] Val F1 : [0.71066] Val acc : [0.89220] Val F1_T : [0.48147] Val F1_F : [0.93985] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:30<00:00,  2.61s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Train Loss : [0.02948] Val Loss : [0.03043] Val F1 : [0.71816] Val acc : [0.89657] Val F1_T : [0.49391] Val F1_F : [0.94240] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:21<00:00,  2.54s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Train Loss : [0.02854] Val Loss : [0.03014] Val F1 : [0.71529] Val acc : [0.91287] Val F1_T : [0.47810] Val F1_F : [0.95247] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:25<00:00,  2.57s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [11] Train Loss : [0.02765] Val Loss : [0.02963] Val F1 : [0.71857] Val acc : [0.89440] Val F1_T : [0.49612] Val F1_F : [0.94102] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:25<00:00,  2.57s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [12] Train Loss : [0.02684] Val Loss : [0.02932] Val F1 : [0.71631] Val acc : [0.89607] Val F1_T : [0.49049] Val F1_F : [0.94214] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:37<00:00,  2.65s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [13] Train Loss : [0.02598] Val Loss : [0.02936] Val F1 : [0.73023] Val acc : [0.90561] Val F1_T : [0.51272] Val F1_F : [0.94775] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:25<00:00,  2.57s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [14] Train Loss : [0.02514] Val Loss : [0.03001] Val F1 : [0.71391] Val acc : [0.90239] Val F1_T : [0.48170] Val F1_F : [0.94612] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:28<00:00,  2.59s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [15] Train Loss : [0.02425] Val Loss : [0.02934] Val F1 : [0.72489] Val acc : [0.91347] Val F1_T : [0.49711] Val F1_F : [0.95266] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:29<00:00,  2.59s/it]\n",
      "100%|██████████| 38/38 [00:43<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [16] Train Loss : [0.02314] Val Loss : [0.02948] Val F1 : [0.71560] Val acc : [0.90189] Val F1_T : [0.48543] Val F1_F : [0.94578] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:31<00:00,  2.61s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [17] Train Loss : [0.02209] Val Loss : [0.03009] Val F1 : [0.71405] Val acc : [0.89351] Val F1_T : [0.48752] Val F1_F : [0.94058] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:27<00:00,  2.58s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [18] Train Loss : [0.02078] Val Loss : [0.03229] Val F1 : [0.70784] Val acc : [0.89935] Val F1_T : [0.47130] Val F1_F : [0.94438] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:27<00:00,  2.58s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [19] Train Loss : [0.01922] Val Loss : [0.03161] Val F1 : [0.70486] Val acc : [0.89511] Val F1_T : [0.46790] Val F1_F : [0.94182] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:19<00:00,  2.53s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [20] Train Loss : [0.01765] Val Loss : [0.03554] Val F1 : [0.70065] Val acc : [0.88903] Val F1_T : [0.46318] Val F1_F : [0.93812] \n",
      "Best Validation F1 Score : [0.73023]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 119/119 [03:26<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "152648it [03:57, 643.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38162it [00:59, 646.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 150/150 [03:36<00:00,  1.44s/it]\n",
      "100%|██████████| 38/38 [00:31<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [0.04931] Val Loss : [0.04954] Val F1 : [0.56674] Val acc : [0.71511] Val F1_T : [0.31320] Val F1_F : [0.82028] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [05:48<00:00,  2.32s/it]\n",
      "100%|██████████| 38/38 [00:42<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [0.04439] Val Loss : [0.04496] Val F1 : [0.61655] Val acc : [0.78673] Val F1_T : [0.36110] Val F1_F : [0.87200] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:13<00:00,  2.49s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [0.04138] Val Loss : [0.04228] Val F1 : [0.64010] Val acc : [0.81513] Val F1_T : [0.38912] Val F1_F : [0.89108] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:15<00:00,  2.50s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train Loss : [0.03892] Val Loss : [0.03822] Val F1 : [0.67335] Val acc : [0.85357] Val F1_T : [0.43073] Val F1_F : [0.91598] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:21<00:00,  2.54s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train Loss : [0.03664] Val Loss : [0.03451] Val F1 : [0.69595] Val acc : [0.87957] Val F1_T : [0.45968] Val F1_F : [0.93223] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:16<00:00,  2.51s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Train Loss : [0.03451] Val Loss : [0.03434] Val F1 : [0.69916] Val acc : [0.88043] Val F1_T : [0.46563] Val F1_F : [0.93268] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:13<00:00,  2.49s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Train Loss : [0.03281] Val Loss : [0.03468] Val F1 : [0.69822] Val acc : [0.87640] Val F1_T : [0.46634] Val F1_F : [0.93010] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:14<00:00,  2.50s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Train Loss : [0.03112] Val Loss : [0.03459] Val F1 : [0.68798] Val acc : [0.87923] Val F1_T : [0.44369] Val F1_F : [0.93226] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:15<00:00,  2.50s/it]\n",
      "100%|██████████| 38/38 [00:41<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Train Loss : [0.02994] Val Loss : [0.03013] Val F1 : [0.72396] Val acc : [0.91316] Val F1_T : [0.49543] Val F1_F : [0.95249] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:10<00:00,  2.47s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Train Loss : [0.02878] Val Loss : [0.03090] Val F1 : [0.70341] Val acc : [0.88227] Val F1_T : [0.47309] Val F1_F : [0.93373] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:18<00:00,  2.53s/it]\n",
      "100%|██████████| 38/38 [00:36<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [11] Train Loss : [0.02799] Val Loss : [0.02927] Val F1 : [0.73061] Val acc : [0.90698] Val F1_T : [0.51263] Val F1_F : [0.94858] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:09<00:00,  2.46s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [12] Train Loss : [0.02708] Val Loss : [0.02905] Val F1 : [0.72722] Val acc : [0.91083] Val F1_T : [0.50343] Val F1_F : [0.95102] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:18<00:00,  2.52s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [13] Train Loss : [0.02640] Val Loss : [0.02894] Val F1 : [0.72425] Val acc : [0.90695] Val F1_T : [0.49979] Val F1_F : [0.94870] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:11<00:00,  2.48s/it]\n",
      "100%|██████████| 38/38 [00:36<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [14] Train Loss : [0.02588] Val Loss : [0.02975] Val F1 : [0.69850] Val acc : [0.92435] Val F1_T : [0.43756] Val F1_F : [0.95945] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:16<00:00,  2.51s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [15] Train Loss : [0.02503] Val Loss : [0.03044] Val F1 : [0.70224] Val acc : [0.87490] Val F1_T : [0.47550] Val F1_F : [0.92898] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:13<00:00,  2.49s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [16] Train Loss : [0.02419] Val Loss : [0.03092] Val F1 : [0.69712] Val acc : [0.87239] Val F1_T : [0.46671] Val F1_F : [0.92752] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:13<00:00,  2.49s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [17] Train Loss : [0.02290] Val Loss : [0.03016] Val F1 : [0.71416] Val acc : [0.88887] Val F1_T : [0.49069] Val F1_F : [0.93763] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:08<00:00,  2.45s/it]\n",
      "100%|██████████| 38/38 [00:36<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [18] Train Loss : [0.02195] Val Loss : [0.03283] Val F1 : [0.71344] Val acc : [0.89938] Val F1_T : [0.48262] Val F1_F : [0.94427] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:16<00:00,  2.51s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [19] Train Loss : [0.02045] Val Loss : [0.03248] Val F1 : [0.71248] Val acc : [0.90438] Val F1_T : [0.47759] Val F1_F : [0.94737] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:10<00:00,  2.47s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [20] Train Loss : [0.01889] Val Loss : [0.03329] Val F1 : [0.70924] Val acc : [0.89610] Val F1_T : [0.47615] Val F1_F : [0.94233] \n",
      "Best Validation F1 Score : [0.73061]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 119/119 [03:24<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/usr/local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "152648it [03:56, 646.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38162it [01:00, 634.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 150/150 [03:34<00:00,  1.43s/it]\n",
      "100%|██████████| 38/38 [00:31<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [0.05110] Val Loss : [0.04170] Val F1 : [0.65633] Val acc : [0.84304] Val F1_T : [0.40303] Val F1_F : [0.90964] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [05:40<00:00,  2.27s/it]\n",
      "100%|██████████| 38/38 [00:34<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [0.04547] Val Loss : [0.04357] Val F1 : [0.62805] Val acc : [0.80190] Val F1_T : [0.37376] Val F1_F : [0.88234] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:03<00:00,  2.42s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [0.04208] Val Loss : [0.03563] Val F1 : [0.69982] Val acc : [0.88711] Val F1_T : [0.46271] Val F1_F : [0.93693] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:01<00:00,  2.41s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train Loss : [0.03927] Val Loss : [0.03710] Val F1 : [0.68689] Val acc : [0.87008] Val F1_T : [0.44739] Val F1_F : [0.92639] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [05:59<00:00,  2.40s/it]\n",
      "100%|██████████| 38/38 [00:40<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train Loss : [0.03679] Val Loss : [0.03361] Val F1 : [0.70297] Val acc : [0.91127] Val F1_T : [0.45422] Val F1_F : [0.95171] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:05<00:00,  2.43s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Train Loss : [0.03451] Val Loss : [0.03176] Val F1 : [0.70642] Val acc : [0.89429] Val F1_T : [0.47157] Val F1_F : [0.94127] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:09<00:00,  2.46s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Train Loss : [0.03267] Val Loss : [0.03225] Val F1 : [0.70873] Val acc : [0.89448] Val F1_T : [0.47613] Val F1_F : [0.94133] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:11<00:00,  2.48s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Train Loss : [0.03108] Val Loss : [0.03095] Val F1 : [0.71231] Val acc : [0.90137] Val F1_T : [0.47910] Val F1_F : [0.94553] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:14<00:00,  2.50s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Train Loss : [0.02967] Val Loss : [0.03064] Val F1 : [0.71864] Val acc : [0.91319] Val F1_T : [0.48468] Val F1_F : [0.95260] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [05:59<00:00,  2.40s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Train Loss : [0.02852] Val Loss : [0.02974] Val F1 : [0.71582] Val acc : [0.90003] Val F1_T : [0.48702] Val F1_F : [0.94462] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:12<00:00,  2.48s/it]\n",
      "100%|██████████| 38/38 [00:35<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [11] Train Loss : [0.02755] Val Loss : [0.02961] Val F1 : [0.71518] Val acc : [0.90643] Val F1_T : [0.48179] Val F1_F : [0.94857] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:11<00:00,  2.48s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [12] Train Loss : [0.02679] Val Loss : [0.02931] Val F1 : [0.72058] Val acc : [0.91159] Val F1_T : [0.48956] Val F1_F : [0.95160] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:07<00:00,  2.45s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [13] Train Loss : [0.02589] Val Loss : [0.02983] Val F1 : [0.72206] Val acc : [0.90905] Val F1_T : [0.49410] Val F1_F : [0.95003] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:05<00:00,  2.43s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [14] Train Loss : [0.02500] Val Loss : [0.02929] Val F1 : [0.71760] Val acc : [0.89896] Val F1_T : [0.49129] Val F1_F : [0.94391] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:11<00:00,  2.48s/it]\n",
      "100%|██████████| 38/38 [00:36<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [15] Train Loss : [0.02416] Val Loss : [0.03079] Val F1 : [0.71015] Val acc : [0.91685] Val F1_T : [0.46537] Val F1_F : [0.95492] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:09<00:00,  2.46s/it]\n",
      "100%|██████████| 38/38 [00:37<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [16] Train Loss : [0.02315] Val Loss : [0.02882] Val F1 : [0.72236] Val acc : [0.90527] Val F1_T : [0.49701] Val F1_F : [0.94771] \n",
      "Model Saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:07<00:00,  2.45s/it]\n",
      "100%|██████████| 38/38 [00:35<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [17] Train Loss : [0.02198] Val Loss : [0.03293] Val F1 : [0.65747] Val acc : [0.82784] Val F1_T : [0.41590] Val F1_F : [0.89904] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 150/150 [06:17<00:00,  2.52s/it]\n",
      "100%|██████████| 38/38 [00:35<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [18] Train Loss : [0.02052] Val Loss : [0.03054] Val F1 : [0.70665] Val acc : [0.90540] Val F1_T : [0.46519] Val F1_F : [0.94811] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:09<00:00,  2.47s/it]\n",
      "100%|██████████| 38/38 [00:38<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [19] Train Loss : [0.01924] Val Loss : [0.03172] Val F1 : [0.70150] Val acc : [0.90677] Val F1_T : [0.45396] Val F1_F : [0.94903] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:15<00:00,  2.50s/it]\n",
      "100%|██████████| 38/38 [00:39<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [20] Train Loss : [0.01776] Val Loss : [0.03817] Val F1 : [0.67444] Val acc : [0.85845] Val F1_T : [0.42969] Val F1_F : [0.91919] \n",
      "Best Validation F1 Score : [0.72236]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model/bert were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 119/119 [03:26<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5,random_state=48,shuffle=True)\n",
    "split = 1\n",
    "results = []\n",
    "\n",
    "for trn_idx, val_idx in kf.split(train_df):\n",
    "    trn, val = train_df.iloc[trn_idx], train_df.iloc[val_idx]\n",
    "    \n",
    "    train_epitope_ids_list, train_epitope_mask_list, train_protein_features, train_label_list = get_preprocessing('train', trn, tokenizer)\n",
    "    val_epitope_ids_list, val_epitope_mask_list, val_protein_features, val_label_list = get_preprocessing('val', val, tokenizer)\n",
    "\n",
    "    standard_scaler = preprocessing.StandardScaler()\n",
    "    standard_scaler = standard_scaler.fit(train_protein_features)\n",
    "\n",
    "    train_protein_features = standard_scaler.transform(train_protein_features)\n",
    "    val_protein_features = standard_scaler.transform(val_protein_features)\n",
    "\n",
    "    train_dataset = CustomDataset(train_epitope_ids_list, train_epitope_mask_list, train_protein_features, train_label_list)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "\n",
    "    val_dataset = CustomDataset(val_epitope_ids_list, val_epitope_mask_list, val_protein_features, val_label_list)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])\n",
    "    \n",
    "    model = TransformerModel()\n",
    "    model = nn.DataParallel(model, device_ids=[4, 5, 6])\n",
    "    model.eval()\n",
    "\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000*CFG['EPOCHS'], eta_min=0)\n",
    "\n",
    "    best_score = train(model, optimizer, train_loader, val_loader, scheduler, device, split)\n",
    "    print(f'Best Validation F1 Score : [{best_score:.5f}]')\n",
    "    \n",
    "    test_pf = standard_scaler.transform(test_protein_features)\n",
    "    \n",
    "    test_dataset = CustomDataset(test_epitope_ids_list, test_epitope_mask_list, test_pf, None)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])\n",
    "    \n",
    "    model = TransformerModel()\n",
    "    best_checkpoint = torch.load(f'epitope_best_model_{split}.pth', map_location=device)\n",
    "    model.load_state_dict(best_checkpoint)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    preds = inference(model, test_loader, device)\n",
    "    results.append(preds)\n",
    "    \n",
    "    submit = pd.read_csv('data/sample_submission.csv')\n",
    "    submit['label'] = preds\n",
    "\n",
    "    submit.to_csv(f'submission/epitope_submission_{split}.csv', index=False)\n",
    "    print('Done.')\n",
    "    \n",
    "    split += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb4717",
   "metadata": {},
   "source": [
    "# prediction 값 hard voting 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "647baebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "preds = np.sum(results, axis=0)\n",
    "voting = np.array([1 if i > 2 else 0 for i in preds])\n",
    "\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit['label'] = voting\n",
    "\n",
    "submit.to_csv(f'submission/epitope_submission_ensemble.csv', index=False)\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
