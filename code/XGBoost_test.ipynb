{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d83a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "import optuna\n",
    "from xgboost import XGBClassifier\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6a71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease dic 생성\n",
    "def get_dic(data):\n",
    "    vocab = {}\n",
    "    for name in data:\n",
    "        if name not in vocab:\n",
    "            vocab[name]=0\n",
    "        vocab[name] += 1\n",
    "    vocab_sorted = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "    token_dic = {}\n",
    "    i = 1\n",
    "    # train에서 보지 않은 disease들 Unknown 토큰 처리\n",
    "    # train과정에서 없는 disease들은 0으로 처리하였음\n",
    "    token_dic['Unknown'] = 0\n",
    "    for (name, freq) in vocab_sorted:\n",
    "        token_dic[name] = i\n",
    "        i += 1\n",
    "    return token_dic\n",
    "\n",
    "def dic_except(dic, a):\n",
    "    try:\n",
    "        return dic[a]\n",
    "    except:\n",
    "        return dic['Unknown']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eca237",
   "metadata": {},
   "source": [
    "# Epitope Peptide Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b886cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 7가지 peptide features를 계산\n",
    "\n",
    "def get_peptide_feature(seq): # CTD descriptor\n",
    "    CTD = {'hydrophobicity': {1: ['R', 'K', 'E', 'D', 'Q', 'N'], 2: ['G', 'A', 'S', 'T', 'P', 'H', 'Y'], 3: ['C', 'L', 'V', 'I', 'M', 'F', 'W']},\n",
    "           'normalized.van.der.waals': {1: ['G', 'A', 'S', 'T', 'P', 'D', 'C'], 2: ['N', 'V', 'E', 'Q', 'I', 'L'], 3: ['M', 'H', 'K', 'F', 'R', 'Y', 'W']},\n",
    "           'polarity': {1: ['L', 'I', 'F', 'W', 'C', 'M', 'V', 'Y'], 2: ['P', 'A', 'T', 'G', 'S'], 3: ['H', 'Q', 'R', 'K', 'N', 'E', 'D']},\n",
    "           'polarizability': {1: ['G', 'A', 'S', 'D', 'T'], 2: ['C', 'P', 'N', 'V', 'E', 'Q', 'I', 'L'], 3: ['K', 'M', 'H', 'F', 'R', 'Y', 'W']},\n",
    "           'charge': {1: ['K', 'R'], 2: ['A', 'N', 'C', 'Q', 'G', 'H', 'I', 'L', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V'], 3: ['D', 'E']},\n",
    "           'secondary': {1: ['E', 'A', 'L', 'M', 'Q', 'K', 'R', 'H'], 2: ['V', 'I', 'Y', 'C', 'W', 'F', 'T'], 3: ['G', 'N', 'P', 'S', 'D']},\n",
    "           'solvent': {1: ['A', 'L', 'F', 'C', 'G', 'I', 'V', 'W'], 2: ['R', 'K', 'Q', 'E', 'N', 'D'], 3: ['M', 'S', 'P', 'T', 'H', 'Y']}}\n",
    "    \n",
    "    seq = str(seq)\n",
    "    sequencelength = len(seq)\n",
    "    Sequence_group = []\n",
    "    \n",
    "    for AAproperty in CTD:\n",
    "        propvalues = \"\"\n",
    "        for letter in seq:\n",
    "            if letter in CTD[AAproperty][1]:\n",
    "                propvalues += \"1\"\n",
    "            elif letter in CTD[AAproperty][2]:\n",
    "                propvalues += \"2\"\n",
    "            elif letter in CTD[AAproperty][3]:\n",
    "                propvalues += \"3\"\n",
    "        abpos_1 = [i for i in range(len(propvalues)) if propvalues.startswith(\"1\", i)]\n",
    "        abpos_1 = [x+1 for x in abpos_1]\n",
    "        abpos_1.insert(0, \"-\")\n",
    "        abpos_2 = [i for i in range(len(propvalues)) if propvalues.startswith(\"2\", i)]\n",
    "        abpos_2 = [x+1 for x in abpos_2]\n",
    "        abpos_2.insert(0, \"-\")\n",
    "        abpos_3 = [i for i in range(len(propvalues)) if propvalues.startswith(\"3\", i)]\n",
    "        abpos_3 = [x+1 for x in abpos_3]\n",
    "        abpos_3.insert(0, \"-\")\n",
    "        property_group1_length = propvalues.count(\"1\")\n",
    "        \n",
    "        if property_group1_length == 0:\n",
    "            Sequence_group.extend([0, 0, 0, 0, 0])\n",
    "        elif property_group1_length == 1:\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "        elif property_group1_length == 2:\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.5*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.75*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[property_group1_length]/sequencelength)*100)\n",
    "        else:\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.25*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.5*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.75*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[property_group1_length]/sequencelength)*100)\n",
    "\n",
    "        property_group2_length = propvalues.count(\"2\")\n",
    "        if property_group2_length == 0:\n",
    "            Sequence_group.extend([0, 0, 0, 0, 0])\n",
    "        elif property_group2_length == 1:\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "        elif property_group2_length == 2:\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.5*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.75*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[property_group2_length]/sequencelength)*100)\n",
    "        else:\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.25*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.5*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.75*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[property_group2_length]/sequencelength)*100)\n",
    "\n",
    "        property_group3_length = propvalues.count(\"3\")\n",
    "        if property_group3_length == 0:\n",
    "            Sequence_group.extend([0, 0, 0, 0, 0])\n",
    "        elif property_group3_length == 1:\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "        elif property_group3_length == 2:\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.5*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.75*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[property_group3_length]/sequencelength)*100)\n",
    "        else:\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.25*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.5*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.75*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[property_group3_length]/sequencelength)*100)\n",
    "    return Sequence_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b85be",
   "metadata": {},
   "source": [
    "# Antigen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044eced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bio python 툴을 이용하여 Antigen의 4가지 features extraction\n",
    "\n",
    "def get_protein_feature(seq):\n",
    "    protein_feature = []\n",
    "    protein_feature.append(ProteinAnalysis(seq).isoelectric_point())\n",
    "    protein_feature.append(ProteinAnalysis(seq).aromaticity())\n",
    "    protein_feature.append(ProteinAnalysis(seq).gravy())\n",
    "    protein_feature.append(ProteinAnalysis(seq).instability_index())\n",
    "    return protein_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89ffae",
   "metadata": {},
   "source": [
    "# Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8620b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_preprocessing(data_type, new_df):   \n",
    "    protein_features = []\n",
    "    epitope_features = []\n",
    "    disease_features = []\n",
    "        \n",
    "    for epitope, antigen, d_type, d_state in tqdm(zip(new_df['epitope_seq'], new_df['antigen_seq'], new_df['disease_type'], new_df['disease_state'])):        \n",
    "\n",
    "        protein_features.append(get_protein_feature(antigen))\n",
    "        epitope_features.append(get_peptide_feature(epitope))\n",
    "        disease_features.append([d_type, d_state])\n",
    "    \n",
    "    label_list = None\n",
    "    if data_type != 'test':\n",
    "        label_list = []\n",
    "        for label in new_df['label']:\n",
    "            label_list.append(label)\n",
    "    print(f'{data_type} dataframe preprocessing was done.')\n",
    "    return protein_features, epitope_features, disease_features, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c88c2",
   "metadata": {},
   "source": [
    "# Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "569d1774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "190811it [06:13, 510.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "dic_disease_type = get_dic(train['disease_type'])\n",
    "dic_disease_state = get_dic(train['disease_state'])\n",
    "train['disease_type'] = train['disease_type'].map(lambda a: dic_except(dic_disease_type, a))\n",
    "train['disease_state'] = train['disease_state'].map(lambda a: dic_except(dic_disease_state, a))\n",
    "protein_features, epitope_features, disease_features, label_list = get_preprocessing('train', train)\n",
    "protein_features = np.array(protein_features)\n",
    "epitope_features = np.array(epitope_features)\n",
    "disease_features = np.array(disease_features)\n",
    "X = np.concatenate((protein_features, epitope_features, disease_features), axis=1)\n",
    "y = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c38f993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120944it [25:59, 77.55it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataframe preprocessing was done.\n"
     ]
    }
   ],
   "source": [
    "# test dataset 불러오기\n",
    "\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "#test data의 disease들 중 train과정에서 보지 않은 disease는 'unknown' 토큰인 0으로 처리\n",
    "test_df['disease_type'] = test_df['disease_type'].map(lambda a: dic_except(dic_disease_type, a))\n",
    "test_df['disease_state'] = test_df['disease_state'].map(lambda a: dic_except(dic_disease_state, a))\n",
    "\n",
    "test_protein_features, test_epitope_features, test_disease_features, label_list = get_preprocessing('test', test_df)\n",
    "\n",
    "test_protein_features = np.array(test_protein_features)\n",
    "test_epitope_features = np.array(test_epitope_features)\n",
    "test_disease_features = np.array(test_disease_features)\n",
    "X_test = np.concatenate((test_protein_features, test_epitope_features, test_disease_features), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a5e8e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589fb68",
   "metadata": {},
   "source": [
    "### Best trial: {'max_depth': 16, 'learning_rate': 0.0025640961747953163, 'n_estimators': 1546, 'subsample': 0.7352711832885261, 'min_child_weight': 1, 'alpha': 0.027937140843060745}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31200a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.027937140843060745, base_score=0.5, booster='gbtree',\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              gamma=0, gpu_id=-1, importance_type='gain',\n",
       "              interaction_constraints='', learning_rate=0.0025640961747953163,\n",
       "              max_delta_step=0, max_depth=16, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=1546, n_jobs=0,\n",
       "              num_parallel_tree=1, random_state=0, reg_alpha=0.0279371403,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=0.7352711832885261,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 탐색한 최적의 하이퍼 파라미터 기반 전체 train dataset을 이용하여 XGBoost train\n",
    "model = XGBClassifier(\n",
    "max_depth = 16,\n",
    "learning_rate = 0.0025640961747953163,\n",
    "n_estimators= 1546,\n",
    "subsample= 0.7352711832885261,\n",
    "min_child_weight= 1,\n",
    "alpha= 0.027937140843060745)\n",
    "model.fit(\n",
    "    np.array(X),\n",
    "    np.array(y),\n",
    "    eval_set=[(np.array(X), np.array(y))],\n",
    "    early_stopping_rounds=100,\n",
    "    eval_metric = 'logloss',\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d03ca0",
   "metadata": {},
   "source": [
    "### → feature 110 (disease_type), feature 109 (disease_state) 가 가장 높은 feature importance를 보인다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c9240",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1897e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# test dataset을 이용하여 최종 Inference\n",
    "\n",
    "preds_all = model.predict(np.array(X_test))\n",
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "submit['label'] = preds_all\n",
    "submit.to_csv('../data/XGBoost.csv', index=False)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb8a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b65d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87694d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7022b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc3404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c716fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193daffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ba00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2bcec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d9a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea5079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
