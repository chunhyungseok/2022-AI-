{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ef14dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35843571",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b627df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dic(data):\n",
    "    vocab = {}\n",
    "    for name in data:\n",
    "        if name not in vocab:\n",
    "            vocab[name]=0\n",
    "        vocab[name] += 1\n",
    "    vocab_sorted = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "    token_dic = {}\n",
    "    i = 1\n",
    "    token_dic['Unknown'] = 0\n",
    "    for (name, freq) in vocab_sorted:\n",
    "        token_dic[name] = i\n",
    "        i += 1\n",
    "    return token_dic\n",
    "\n",
    "def dic_except(dic, a):\n",
    "    try:\n",
    "        return dic[a]\n",
    "    except:\n",
    "        return dic['Unknown']\n",
    "\n",
    "dic_disease_type = get_dic(train['disease_type'])\n",
    "dic_disease_state = get_dic(train['disease_state'])\n",
    "\n",
    "train['disease_type'] = train['disease_type'].map(lambda a: dic_except(dic_disease_type, a))\n",
    "train['disease_state'] = train['disease_state'].map(lambda a: dic_except(dic_disease_state, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b48ea831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peptide_feature(seq): # CTD descriptor\n",
    "    CTD = {'hydrophobicity': {1: ['R', 'K', 'E', 'D', 'Q', 'N'], 2: ['G', 'A', 'S', 'T', 'P', 'H', 'Y'], 3: ['C', 'L', 'V', 'I', 'M', 'F', 'W']},\n",
    "           'normalized.van.der.waals': {1: ['G', 'A', 'S', 'T', 'P', 'D', 'C'], 2: ['N', 'V', 'E', 'Q', 'I', 'L'], 3: ['M', 'H', 'K', 'F', 'R', 'Y', 'W']},\n",
    "           'polarity': {1: ['L', 'I', 'F', 'W', 'C', 'M', 'V', 'Y'], 2: ['P', 'A', 'T', 'G', 'S'], 3: ['H', 'Q', 'R', 'K', 'N', 'E', 'D']},\n",
    "           'polarizability': {1: ['G', 'A', 'S', 'D', 'T'], 2: ['C', 'P', 'N', 'V', 'E', 'Q', 'I', 'L'], 3: ['K', 'M', 'H', 'F', 'R', 'Y', 'W']},\n",
    "           'charge': {1: ['K', 'R'], 2: ['A', 'N', 'C', 'Q', 'G', 'H', 'I', 'L', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V'], 3: ['D', 'E']},\n",
    "           'secondary': {1: ['E', 'A', 'L', 'M', 'Q', 'K', 'R', 'H'], 2: ['V', 'I', 'Y', 'C', 'W', 'F', 'T'], 3: ['G', 'N', 'P', 'S', 'D']},\n",
    "           'solvent': {1: ['A', 'L', 'F', 'C', 'G', 'I', 'V', 'W'], 2: ['R', 'K', 'Q', 'E', 'N', 'D'], 3: ['M', 'S', 'P', 'T', 'H', 'Y']}}\n",
    "    \n",
    "    seq = str(seq)\n",
    "    sequencelength = len(seq)\n",
    "    Sequence_group = []\n",
    "    \n",
    "    for AAproperty in CTD:\n",
    "        propvalues = \"\"\n",
    "        for letter in seq:\n",
    "            if letter in CTD[AAproperty][1]:\n",
    "                propvalues += \"1\"\n",
    "            elif letter in CTD[AAproperty][2]:\n",
    "                propvalues += \"2\"\n",
    "            elif letter in CTD[AAproperty][3]:\n",
    "                propvalues += \"3\"\n",
    "        abpos_1 = [i for i in range(len(propvalues)) if propvalues.startswith(\"1\", i)]\n",
    "        abpos_1 = [x+1 for x in abpos_1]\n",
    "        abpos_1.insert(0, \"-\")\n",
    "        abpos_2 = [i for i in range(len(propvalues)) if propvalues.startswith(\"2\", i)]\n",
    "        abpos_2 = [x+1 for x in abpos_2]\n",
    "        abpos_2.insert(0, \"-\")\n",
    "        abpos_3 = [i for i in range(len(propvalues)) if propvalues.startswith(\"3\", i)]\n",
    "        abpos_3 = [x+1 for x in abpos_3]\n",
    "        abpos_3.insert(0, \"-\")\n",
    "        property_group1_length = propvalues.count(\"1\")\n",
    "        \n",
    "        if property_group1_length == 0:\n",
    "            Sequence_group.extend([0, 0, 0, 0, 0])\n",
    "        elif property_group1_length == 1:\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "        elif property_group1_length == 2:\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.5*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.75*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[property_group1_length]/sequencelength)*100)\n",
    "        else:\n",
    "            Sequence_group.append((abpos_1[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.25*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.5*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[round((0.75*property_group1_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_1[property_group1_length]/sequencelength)*100)\n",
    "\n",
    "        property_group2_length = propvalues.count(\"2\")\n",
    "        if property_group2_length == 0:\n",
    "            Sequence_group.extend([0, 0, 0, 0, 0])\n",
    "        elif property_group2_length == 1:\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "        elif property_group2_length == 2:\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.5*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.75*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[property_group2_length]/sequencelength)*100)\n",
    "        else:\n",
    "            Sequence_group.append((abpos_2[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.25*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.5*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[round((0.75*property_group2_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_2[property_group2_length]/sequencelength)*100)\n",
    "\n",
    "        property_group3_length = propvalues.count(\"3\")\n",
    "        if property_group3_length == 0:\n",
    "            Sequence_group.extend([0, 0, 0, 0, 0])\n",
    "        elif property_group3_length == 1:\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "        elif property_group3_length == 2:\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.5*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.75*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[property_group3_length]/sequencelength)*100)\n",
    "        else:\n",
    "            Sequence_group.append((abpos_3[1]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.25*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.5*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[round((0.75*property_group3_length)-0.1)]/sequencelength)*100)\n",
    "            Sequence_group.append((abpos_3[property_group3_length]/sequencelength)*100)\n",
    "    return Sequence_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb3b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_feature(seq):\n",
    "    protein_feature = []\n",
    "    protein_feature.append(ProteinAnalysis(seq).isoelectric_point())\n",
    "    protein_feature.append(ProteinAnalysis(seq).aromaticity())\n",
    "    protein_feature.append(ProteinAnalysis(seq).gravy())\n",
    "    protein_feature.append(ProteinAnalysis(seq).instability_index())\n",
    "    return protein_feature\n",
    "    \n",
    "def get_preprocessing(data_type, new_df):   \n",
    "    protein_features = []\n",
    "    epitope_features = []\n",
    "    disease_features = []\n",
    "        \n",
    "    for epitope, antigen, d_type, d_state in tqdm(zip(new_df['epitope_seq'], new_df['antigen_seq'], new_df['disease_type'], new_df['disease_state'])):        \n",
    "\n",
    "        protein_features.append(get_protein_feature(antigen))\n",
    "        epitope_features.append(get_peptide_feature(epitope))\n",
    "        disease_features.append([d_type, d_state])\n",
    "    \n",
    "    label_list = None\n",
    "    if data_type != 'test':\n",
    "        label_list = []\n",
    "        for label in new_df['label']:\n",
    "            label_list.append(label)\n",
    "    print(f'{data_type} dataframe preprocessing was done.')\n",
    "    return protein_features, epitope_features, disease_features, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05b1516e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152648it [05:04, 501.91it/s]\n",
      "33it [00:00, 329.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38163it [01:16, 500.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(train, train_size=0.8, random_state=12)\n",
    "\n",
    "train_protein_features, train_epitope_features, train_disease_features, train_label_list = get_preprocessing('train', train)\n",
    "val_protein_features, val_epitope_features, val_disease_features, val_label_list = get_preprocessing('val', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ec6e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_protein_features = np.array(train_protein_features)\n",
    "train_epitope_features = np.array(train_epitope_features)\n",
    "train_disease_features = np.array(train_disease_features)\n",
    "X_train = np.concatenate((train_protein_features, train_epitope_features, train_disease_features), axis=1)\n",
    "y_train = np.array(train_label_list)\n",
    "\n",
    "val_protein_features = np.array(val_protein_features)\n",
    "val_epitope_features = np.array(val_epitope_features)\n",
    "val_disease_features = np.array(val_disease_features)\n",
    "X_val = np.concatenate((val_protein_features, val_epitope_features, val_disease_features), axis=1)\n",
    "y_val = np.array(val_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64caed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152648it [05:00, 507.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "protein_features, epitope_features, disease_features, label_list = get_preprocessing('train', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9ad2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_features = np.array(protein_features)\n",
    "epitope_features = np.array(epitope_features)\n",
    "disease_features = np.array(disease_features)\n",
    "X = np.concatenate((protein_features, epitope_features, disease_features), axis=1)\n",
    "y = np.array(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903fad7",
   "metadata": {},
   "source": [
    "# Disease 정보 사용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "60d76f37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-26 17:14:27,639]\u001b[0m A new study created in memory with name: lgbm_parameter_opt\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:15:02,735]\u001b[0m Trial 0 finished with value: 0.13750658818480216 and parameters: {'reg_alpha': 1.12424581642324e-05, 'reg_lambda': 0.08556428806974939, 'max_depth': 15, 'num_leaves': 154, 'colsample_bytree': 0.4936111842654619, 'subsample': 0.40919616423534183, 'subsample_freq': 1, 'min_child_samples': 88, 'max_bin': 380}. Best is trial 0 with value: 0.13750658818480216.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:15:33,695]\u001b[0m Trial 1 finished with value: 0.13683689055745352 and parameters: {'reg_alpha': 2.1245096608103405e-05, 'reg_lambda': 0.0018526142807772773, 'max_depth': 20, 'num_leaves': 214, 'colsample_bytree': 0.5274034664069657, 'subsample': 0.42727747704497043, 'subsample_freq': 2, 'min_child_samples': 34, 'max_bin': 357}. Best is trial 1 with value: 0.13683689055745352.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:16:01,708]\u001b[0m Trial 2 finished with value: 0.1353657018333897 and parameters: {'reg_alpha': 1.2964031109077052e-05, 'reg_lambda': 0.02621062970553237, 'max_depth': 13, 'num_leaves': 37, 'colsample_bytree': 0.5752867891211308, 'subsample': 0.5564532903055841, 'subsample_freq': 5, 'min_child_samples': 80, 'max_bin': 260}. Best is trial 2 with value: 0.1353657018333897.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:16:21,862]\u001b[0m Trial 3 finished with value: 0.19371338206689742 and parameters: {'reg_alpha': 1.5431890808024213e-05, 'reg_lambda': 0.05331731527343814, 'max_depth': 1, 'num_leaves': 156, 'colsample_bytree': 0.502314474212375, 'subsample': 0.3455361150896956, 'subsample_freq': 10, 'min_child_samples': 97, 'max_bin': 443}. Best is trial 2 with value: 0.1353657018333897.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:16:52,950]\u001b[0m Trial 4 finished with value: 0.1329030497498838 and parameters: {'reg_alpha': 9.145366937509386e-06, 'reg_lambda': 0.008790499283853408, 'max_depth': 14, 'num_leaves': 114, 'colsample_bytree': 0.47322294090686734, 'subsample': 0.6466238370778892, 'subsample_freq': 1, 'min_child_samples': 92, 'max_bin': 277}. Best is trial 4 with value: 0.1329030497498838.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:17:27,922]\u001b[0m Trial 5 finished with value: 0.12558372197949474 and parameters: {'reg_alpha': 1.987904330777592e-05, 'reg_lambda': 0.028054003730936226, 'max_depth': 11, 'num_leaves': 141, 'colsample_bytree': 0.5109126733153162, 'subsample': 0.9787092394351908, 'subsample_freq': 8, 'min_child_samples': 95, 'max_bin': 469}. Best is trial 5 with value: 0.12558372197949474.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:18:34,510]\u001b[0m Trial 6 finished with value: 0.1516082304152577 and parameters: {'reg_alpha': 1.7941020364544445e-05, 'reg_lambda': 0.08296868193333816, 'max_depth': 2, 'num_leaves': 51, 'colsample_bytree': 0.4271363733463229, 'subsample': 0.527731231534285, 'subsample_freq': 4, 'min_child_samples': 31, 'max_bin': 449}. Best is trial 5 with value: 0.12558372197949474.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:19:01,949]\u001b[0m Trial 7 finished with value: 0.1403248273864083 and parameters: {'reg_alpha': 1.0709032267540741e-05, 'reg_lambda': 0.025284113062519174, 'max_depth': 11, 'num_leaves': 37, 'colsample_bytree': 0.8813181884524238, 'subsample': 0.35218545057583955, 'subsample_freq': 10, 'min_child_samples': 79, 'max_bin': 259}. Best is trial 5 with value: 0.12558372197949474.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:19:37,482]\u001b[0m Trial 8 finished with value: 0.13708699772869476 and parameters: {'reg_alpha': 1.7560829253683595e-07, 'reg_lambda': 0.07339153040632079, 'max_depth': 15, 'num_leaves': 187, 'colsample_bytree': 0.8627622080115674, 'subsample': 0.35183125621386324, 'subsample_freq': 4, 'min_child_samples': 16, 'max_bin': 459}. Best is trial 5 with value: 0.12558372197949474.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:21:04,400]\u001b[0m Trial 9 finished with value: 0.14974978824282442 and parameters: {'reg_alpha': 1.8702710823558463e-05, 'reg_lambda': 0.02978082892775818, 'max_depth': 2, 'num_leaves': 81, 'colsample_bytree': 0.5951099932160482, 'subsample': 0.8107243248366449, 'subsample_freq': 7, 'min_child_samples': 90, 'max_bin': 342}. Best is trial 5 with value: 0.12558372197949474.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.12558372197949474\n",
      "Best trial: {'reg_alpha': 1.987904330777592e-05, 'reg_lambda': 0.028054003730936226, 'max_depth': 11, 'num_leaves': 141, 'colsample_bytree': 0.5109126733153162, 'subsample': 0.9787092394351908, 'subsample_freq': 8, 'min_child_samples': 95, 'max_bin': 469}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: Trial) -> float:\n",
    "    params_lgb = {\n",
    "        \"random_state\": 42,\n",
    "        \"verbosity\": -1,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"logloss\",\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 3e-5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 9e-2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
    "    }\n",
    "    model = LGBMClassifier(**params_lgb)\n",
    "    model.fit(\n",
    "        np.array(X_train),\n",
    "        np.array(y_train),\n",
    "        eval_set=[(np.array(X_train), np.array(y_train)), (np.array(X_val), np.array(y_val))],\n",
    "        early_stopping_rounds=100,\n",
    "        eval_metric = 'logloss',\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    lgb_pred = model.predict_proba(np.array(X_val))\n",
    "    log_score = log_loss(np.array(y_val), lgb_pred)\n",
    "    \n",
    "#     preds = model.predict(np.array(X_val))\n",
    "#     score = log_loss(np.array(y_val), preds)\n",
    "    \n",
    "    return log_score\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"lgbm_parameter_opt\",\n",
    "    direction=\"minimize\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Best Score:\", study.best_value)\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dd4b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 ==> loss: 0.13731488464999617, f1_score: 0.7991047982185249\n",
      "fold: 2 ==> loss: 0.13217129734908714, f1_score: 0.7910756959459537\n",
      "fold: 3 ==> loss: 0.13265068501508862, f1_score: 0.7956988155859193\n",
      "fold: 4 ==> loss: 0.13263232120105922, f1_score: 0.8017897618502137\n",
      "fold: 5 ==> loss: 0.13464089496787632, f1_score: 0.7935229515406289\n",
      "0.796238404628248\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Best_trial = study.best_trial.params\n",
    "preds = np.zeros((len(X_test), 2))\n",
    "kf = KFold(n_splits=5,random_state=48,shuffle=True)\n",
    "loss=[]  # list contains rmse for each fold\n",
    "f1 = []\n",
    "n=0\n",
    "for trn_idx, test_idx in kf.split(np.array(X), np.array(y)):\n",
    "    X_tr,X_val=np.array(X)[trn_idx],np.array(X)[test_idx]\n",
    "    y_tr,y_val=np.array(y)[trn_idx],np.array(y)[test_idx]\n",
    "    model = LGBMClassifier(**Best_trial)\n",
    "    model.fit(X_tr,y_tr,eval_set=[(np.array(X_train), np.array(y_train)), (np.array(X_val), np.array(y_val))],early_stopping_rounds=100,eval_metric = 'logloss', verbose=False)\n",
    "    preds+=model.predict_proba(X_test)\n",
    "    loss.append(log_loss(y_val, model.predict_proba(X_val)))\n",
    "    f1.append(f1_score(y_val, model.predict(X_val), average='macro'))\n",
    "    print(f\"fold: {n+1} ==> loss: {loss[n]}, f1_score: {f1[n]}\")\n",
    "    n+=1\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b38020b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 ==> loss: 0.13731488464999617, f1_score: 0.6287683031869079\n",
      "fold: 2 ==> loss: 0.13217129734908714, f1_score: 0.611878453038674\n",
      "fold: 3 ==> loss: 0.13265068501508862, f1_score: 0.6212765957446807\n",
      "fold: 4 ==> loss: 0.13263232120105922, f1_score: 0.6331724440544098\n",
      "fold: 5 ==> loss: 0.13464089496787632, f1_score: 0.6167841710256994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Best_trial = study.best_trial.params\n",
    "preds = np.zeros((len(X_test), 2))\n",
    "kf = KFold(n_splits=5,random_state=48,shuffle=True)\n",
    "loss=[]  # list contains rmse for each fold\n",
    "f1 = []\n",
    "n=0\n",
    "for trn_idx, test_idx in kf.split(np.array(X), np.array(y)):\n",
    "    X_tr,X_val=np.array(X)[trn_idx],np.array(X)[test_idx]\n",
    "    y_tr,y_val=np.array(y)[trn_idx],np.array(y)[test_idx]\n",
    "    model = LGBMClassifier(**Best_trial)\n",
    "    model.fit(X_tr,y_tr,eval_set=[(np.array(X_train), np.array(y_train)), (np.array(X_val), np.array(y_val))],early_stopping_rounds=100,eval_metric = 'logloss', verbose=False)\n",
    "    preds+=model.predict_proba(X_test)\n",
    "    loss.append(log_loss(y_val, model.predict_proba(X_val)))\n",
    "    f1.append(f1_score(y_val, model.predict(X_val)))\n",
    "    print(f\"fold: {n+1} ==> loss: {loss[n]}, f1_score: {f1[n]}\")\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d95b0f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 ==> loss: 0.13731488464999617, f1_score: 0.9694412932501417\n",
      "fold: 2 ==> loss: 0.13217129734908714, f1_score: 0.9702729388532335\n",
      "fold: 3 ==> loss: 0.13265068501508862, f1_score: 0.9701210354271579\n",
      "fold: 4 ==> loss: 0.13263232120105922, f1_score: 0.9704070796460177\n",
      "fold: 5 ==> loss: 0.13464089496787632, f1_score: 0.9702617320555584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Best_trial = study.best_trial.params\n",
    "preds = np.zeros((len(X_test), 2))\n",
    "kf = KFold(n_splits=5,random_state=48,shuffle=True)\n",
    "loss=[]  # list contains rmse for each fold\n",
    "f1 = []\n",
    "n=0\n",
    "for trn_idx, test_idx in kf.split(np.array(X), np.array(y)):\n",
    "    X_tr,X_val=np.array(X)[trn_idx],np.array(X)[test_idx]\n",
    "    y_tr,y_val=np.array(y)[trn_idx],np.array(y)[test_idx]\n",
    "    model = LGBMClassifier(**Best_trial)\n",
    "    model.fit(X_tr,y_tr,eval_set=[(np.array(X_train), np.array(y_train)), (np.array(X_val), np.array(y_val))],early_stopping_rounds=100,eval_metric = 'logloss', verbose=False)\n",
    "    preds+=model.predict_proba(X_test)\n",
    "    loss.append(log_loss(y_val, model.predict_proba(X_val)))\n",
    "    f1.append(f1_score(y_val, model.predict(X_val), pos_label=0))\n",
    "    print(f\"fold: {n+1} ==> loss: {loss[n]}, f1_score: {f1[n]}\")\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ee00a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c41b5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 110 (0.114)\n",
      "2. feature 109 (0.060)\n",
      "3. feature 0 (0.030)\n",
      "4. feature 1 (0.029)\n",
      "5. feature 2 (0.027)\n",
      "6. feature 69 (0.024)\n",
      "7. feature 3 (0.022)\n",
      "8. feature 79 (0.014)\n",
      "9. feature 19 (0.011)\n",
      "10. feature 24 (0.010)\n",
      "11. feature 54 (0.010)\n",
      "12. feature 94 (0.010)\n",
      "13. feature 49 (0.010)\n",
      "14. feature 9 (0.009)\n",
      "15. feature 92 (0.008)\n",
      "16. feature 27 (0.008)\n",
      "17. feature 91 (0.008)\n",
      "18. feature 39 (0.008)\n",
      "19. feature 82 (0.008)\n",
      "20. feature 89 (0.008)\n",
      "21. feature 87 (0.008)\n",
      "22. feature 81 (0.008)\n",
      "23. feature 57 (0.007)\n",
      "24. feature 97 (0.007)\n",
      "25. feature 80 (0.007)\n",
      "26. feature 107 (0.007)\n",
      "27. feature 25 (0.007)\n",
      "28. feature 17 (0.007)\n",
      "29. feature 26 (0.007)\n",
      "30. feature 96 (0.007)\n",
      "31. feature 99 (0.007)\n",
      "32. feature 86 (0.007)\n",
      "33. feature 4 (0.007)\n",
      "34. feature 52 (0.007)\n",
      "35. feature 56 (0.007)\n",
      "36. feature 78 (0.007)\n",
      "37. feature 106 (0.007)\n",
      "38. feature 37 (0.007)\n",
      "39. feature 95 (0.007)\n",
      "40. feature 44 (0.007)\n",
      "41. feature 84 (0.007)\n",
      "42. feature 85 (0.007)\n",
      "43. feature 90 (0.007)\n",
      "44. feature 71 (0.007)\n",
      "45. feature 51 (0.007)\n",
      "46. feature 32 (0.007)\n",
      "47. feature 34 (0.007)\n",
      "48. feature 36 (0.007)\n",
      "49. feature 77 (0.007)\n",
      "50. feature 42 (0.007)\n",
      "51. feature 62 (0.007)\n",
      "52. feature 76 (0.007)\n",
      "53. feature 74 (0.007)\n",
      "54. feature 104 (0.007)\n",
      "55. feature 12 (0.007)\n",
      "56. feature 50 (0.007)\n",
      "57. feature 14 (0.007)\n",
      "58. feature 16 (0.007)\n",
      "59. feature 75 (0.007)\n",
      "60. feature 72 (0.006)\n",
      "61. feature 55 (0.006)\n",
      "62. feature 70 (0.006)\n",
      "63. feature 21 (0.006)\n",
      "64. feature 35 (0.006)\n",
      "65. feature 22 (0.006)\n",
      "66. feature 102 (0.006)\n",
      "67. feature 105 (0.006)\n",
      "68. feature 41 (0.006)\n",
      "69. feature 7 (0.006)\n",
      "70. feature 31 (0.006)\n",
      "71. feature 88 (0.006)\n",
      "72. feature 108 (0.006)\n",
      "73. feature 63 (0.006)\n",
      "74. feature 11 (0.006)\n",
      "75. feature 47 (0.006)\n",
      "76. feature 61 (0.006)\n",
      "77. feature 33 (0.006)\n",
      "78. feature 28 (0.006)\n",
      "79. feature 15 (0.006)\n",
      "80. feature 29 (0.006)\n",
      "81. feature 68 (0.006)\n",
      "82. feature 93 (0.006)\n",
      "83. feature 46 (0.006)\n",
      "84. feature 101 (0.006)\n",
      "85. feature 6 (0.006)\n",
      "86. feature 18 (0.006)\n",
      "87. feature 40 (0.006)\n",
      "88. feature 10 (0.006)\n",
      "89. feature 38 (0.006)\n",
      "90. feature 45 (0.006)\n",
      "91. feature 20 (0.006)\n",
      "92. feature 67 (0.006)\n",
      "93. feature 66 (0.006)\n",
      "94. feature 59 (0.006)\n",
      "95. feature 30 (0.006)\n",
      "96. feature 64 (0.006)\n",
      "97. feature 60 (0.006)\n",
      "98. feature 53 (0.005)\n",
      "99. feature 5 (0.005)\n",
      "100. feature 100 (0.005)\n",
      "101. feature 65 (0.005)\n",
      "102. feature 58 (0.005)\n",
      "103. feature 98 (0.005)\n",
      "104. feature 83 (0.005)\n",
      "105. feature 8 (0.005)\n",
      "106. feature 43 (0.005)\n",
      "107. feature 103 (0.005)\n",
      "108. feature 13 (0.005)\n",
      "109. feature 48 (0.005)\n",
      "110. feature 23 (0.005)\n",
      "111. feature 73 (0.003)\n"
     ]
    }
   ],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"{}. feature {} ({:.3f})\".format(f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.title(\"Feature ranking:\")\n",
    "# plt.bar(range(X.shape[1]), importances[indices],\n",
    "#         color='r', yerr=std[indices], align='center', alpha=0.3)\n",
    "# plt.xticks(range(X.shape[1]), X.columns[indices], rotation=45)\n",
    "# plt.xlim([-1, X.shape[1]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b521d35",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94161eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.5109126733153162, max_bin=469, max_depth=11,\n",
       "               min_child_samples=95, num_leaves=141,\n",
       "               reg_alpha=1.987904330777592e-05, reg_lambda=0.028054003730936226,\n",
       "               subsample=0.9787092394351908, subsample_freq=8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMClassifier(**study.best_trial.params)\n",
    "model.fit(\n",
    "    np.array(X),\n",
    "    np.array(y),\n",
    "    eval_set=[(np.array(X), np.array(y))],\n",
    "    early_stopping_rounds=100,\n",
    "    eval_metric = 'logloss',\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb8aceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120944it [26:04, 77.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataframe preprocessing was done.\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df['disease_type'] = test_df['disease_type'].map(lambda a: dic_except(dic_disease_type, a))\n",
    "test_df['disease_state'] = test_df['disease_state'].map(lambda a: dic_except(dic_disease_state, a))\n",
    "\n",
    "test_protein_features, test_epitope_features, test_disease_features, label_list = get_preprocessing('test', test_df)\n",
    "\n",
    "test_protein_features = np.array(test_protein_features)\n",
    "test_epitope_features = np.array(test_epitope_features)\n",
    "test_disease_features = np.array(test_disease_features)\n",
    "X_test = np.concatenate((test_protein_features, test_epitope_features, test_disease_features), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0654dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "preds_all = model.predict(np.array(X_test))\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit['label'] = preds_all\n",
    "submit.to_csv('submission/lgbm_opt_disease.csv', index=False)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ff04250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    112270\n",
       "1      8674\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a88e45",
   "metadata": {},
   "source": [
    "# Disease 정보 사용안한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2bccc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((train_protein_features, train_epitope_features), axis=1)\n",
    "y_train = np.array(train_label_list)\n",
    "\n",
    "val_protein_features = np.array(val_protein_features)\n",
    "val_epitope_features = np.array(val_epitope_features)\n",
    "X_val = np.concatenate((val_protein_features, val_epitope_features), axis=1)\n",
    "y_val = np.array(val_label_list)\n",
    "\n",
    "X = np.concatenate((protein_features, epitope_features), axis=1)\n",
    "y = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f05d2eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-26 17:00:35,762]\u001b[0m A new study created in memory with name: lgbm_parameter_opt\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:01:08,615]\u001b[0m Trial 0 finished with value: 0.16404257992767515 and parameters: {'reg_alpha': 1.12424581642324e-05, 'reg_lambda': 0.08556428806974939, 'max_depth': 15, 'num_leaves': 154, 'colsample_bytree': 0.4936111842654619, 'subsample': 0.40919616423534183, 'subsample_freq': 1, 'min_child_samples': 88, 'max_bin': 380}. Best is trial 0 with value: 0.16404257992767515.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:01:47,006]\u001b[0m Trial 1 finished with value: 0.16184348586830047 and parameters: {'reg_alpha': 2.1245096608103405e-05, 'reg_lambda': 0.0018526142807772773, 'max_depth': 20, 'num_leaves': 214, 'colsample_bytree': 0.5274034664069657, 'subsample': 0.42727747704497043, 'subsample_freq': 2, 'min_child_samples': 34, 'max_bin': 357}. Best is trial 1 with value: 0.16184348586830047.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:02:22,942]\u001b[0m Trial 2 finished with value: 0.160400879359445 and parameters: {'reg_alpha': 1.2964031109077052e-05, 'reg_lambda': 0.02621062970553237, 'max_depth': 13, 'num_leaves': 37, 'colsample_bytree': 0.5752867891211308, 'subsample': 0.5564532903055841, 'subsample_freq': 5, 'min_child_samples': 80, 'max_bin': 260}. Best is trial 2 with value: 0.160400879359445.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:03:07,220]\u001b[0m Trial 3 finished with value: 0.2234976396054171 and parameters: {'reg_alpha': 1.5431890808024213e-05, 'reg_lambda': 0.05331731527343814, 'max_depth': 1, 'num_leaves': 156, 'colsample_bytree': 0.502314474212375, 'subsample': 0.3455361150896956, 'subsample_freq': 10, 'min_child_samples': 97, 'max_bin': 443}. Best is trial 2 with value: 0.160400879359445.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:03:37,924]\u001b[0m Trial 4 finished with value: 0.15795051579002944 and parameters: {'reg_alpha': 9.145366937509386e-06, 'reg_lambda': 0.008790499283853408, 'max_depth': 14, 'num_leaves': 114, 'colsample_bytree': 0.47322294090686734, 'subsample': 0.6466238370778892, 'subsample_freq': 1, 'min_child_samples': 92, 'max_bin': 277}. Best is trial 4 with value: 0.15795051579002944.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:04:15,758]\u001b[0m Trial 5 finished with value: 0.14956090899218366 and parameters: {'reg_alpha': 1.987904330777592e-05, 'reg_lambda': 0.028054003730936226, 'max_depth': 11, 'num_leaves': 141, 'colsample_bytree': 0.5109126733153162, 'subsample': 0.9787092394351908, 'subsample_freq': 8, 'min_child_samples': 95, 'max_bin': 469}. Best is trial 5 with value: 0.14956090899218366.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:05:27,595]\u001b[0m Trial 6 finished with value: 0.17770018129461618 and parameters: {'reg_alpha': 1.7941020364544445e-05, 'reg_lambda': 0.08296868193333816, 'max_depth': 2, 'num_leaves': 51, 'colsample_bytree': 0.4271363733463229, 'subsample': 0.527731231534285, 'subsample_freq': 4, 'min_child_samples': 31, 'max_bin': 449}. Best is trial 5 with value: 0.14956090899218366.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:05:51,784]\u001b[0m Trial 7 finished with value: 0.16789695134023636 and parameters: {'reg_alpha': 1.0709032267540741e-05, 'reg_lambda': 0.025284113062519174, 'max_depth': 11, 'num_leaves': 37, 'colsample_bytree': 0.8813181884524238, 'subsample': 0.35218545057583955, 'subsample_freq': 10, 'min_child_samples': 79, 'max_bin': 259}. Best is trial 5 with value: 0.14956090899218366.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:06:33,594]\u001b[0m Trial 8 finished with value: 0.16165389336392966 and parameters: {'reg_alpha': 1.7560829253683595e-07, 'reg_lambda': 0.07339153040632079, 'max_depth': 15, 'num_leaves': 187, 'colsample_bytree': 0.8627622080115674, 'subsample': 0.35183125621386324, 'subsample_freq': 4, 'min_child_samples': 16, 'max_bin': 459}. Best is trial 5 with value: 0.14956090899218366.\u001b[0m\n",
      "\u001b[32m[I 2022-07-26 17:07:56,683]\u001b[0m Trial 9 finished with value: 0.1766193581353093 and parameters: {'reg_alpha': 1.8702710823558463e-05, 'reg_lambda': 0.02978082892775818, 'max_depth': 2, 'num_leaves': 81, 'colsample_bytree': 0.5951099932160482, 'subsample': 0.8107243248366449, 'subsample_freq': 7, 'min_child_samples': 90, 'max_bin': 342}. Best is trial 5 with value: 0.14956090899218366.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.14956090899218366\n",
      "Best trial: {'reg_alpha': 1.987904330777592e-05, 'reg_lambda': 0.028054003730936226, 'max_depth': 11, 'num_leaves': 141, 'colsample_bytree': 0.5109126733153162, 'subsample': 0.9787092394351908, 'subsample_freq': 8, 'min_child_samples': 95, 'max_bin': 469}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial: Trial) -> float:\n",
    "    params_lgb = {\n",
    "        \"random_state\": 42,\n",
    "        \"verbosity\": -1,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"logloss\",\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 3e-5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 9e-2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 10),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 500),\n",
    "    }\n",
    "    model = LGBMClassifier(**params_lgb)\n",
    "    model.fit(\n",
    "        np.array(X_train),\n",
    "        np.array(y_train),\n",
    "        eval_set=[(np.array(X_train), np.array(y_train)), (np.array(X_val), np.array(y_val))],\n",
    "        early_stopping_rounds=100,\n",
    "        eval_metric = 'logloss',\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    lgb_pred = model.predict_proba(np.array(X_val))\n",
    "    log_score = log_loss(np.array(y_val), lgb_pred)\n",
    "    \n",
    "#     preds = model.predict(np.array(X_val))\n",
    "#     score = log_loss(np.array(y_val), preds)\n",
    "    \n",
    "    return log_score\n",
    "\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name=\"lgbm_parameter_opt\",\n",
    "    direction=\"minimize\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"Best Score:\", study.best_value)\n",
    "print(\"Best trial:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b379c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152648, 109)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9de18cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 ==> loss: 0.16151938553027528, f1_score: 0.7560531038767961\n",
      "fold: 2 ==> loss: 0.15427763369706507, f1_score: 0.7531512319189466\n",
      "fold: 3 ==> loss: 0.1555143084204252, f1_score: 0.7591204076266131\n",
      "fold: 4 ==> loss: 0.15704243865426, f1_score: 0.760549232003195\n",
      "fold: 5 ==> loss: 0.1567530727579862, f1_score: 0.7485983514094695\n",
      "0.755494465367004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Best_trial = study.best_trial.params\n",
    "preds = np.zeros((len(X_test), 2))\n",
    "kf = KFold(n_splits=5,random_state=48,shuffle=True)\n",
    "loss=[]  # list contains rmse for each fold\n",
    "f1 = []\n",
    "n=0\n",
    "for trn_idx, test_idx in kf.split(X, y):\n",
    "    X_tr,X_val=np.array(X)[trn_idx],np.array(X)[test_idx]\n",
    "    y_tr,y_val=np.array(y)[trn_idx],np.array(y)[test_idx]\n",
    "    model = LGBMClassifier(**Best_trial)\n",
    "    model.fit(X_tr,y_tr,eval_set=[(np.array(X_train), np.array(y_train)), (np.array(X_val), np.array(y_val))],early_stopping_rounds=100,eval_metric = 'logloss', verbose=False)\n",
    "#     preds+=model.predict_proba(X_test)\n",
    "    loss.append(log_loss(y_val, model.predict_proba(X_val)))\n",
    "    f1.append(f1_score(y_val, model.predict(X_val), average='macro'))\n",
    "    print(f\"fold: {n+1} ==> loss: {loss[n]}, f1_score: {f1[n]}\")\n",
    "    n+=1\n",
    "print(np.mean(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16625710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 ==> loss: 0.16151938553027528, f1_score: 0.546632723906503\n",
      "fold: 2 ==> loss: 0.15427763369706507, f1_score: 0.5388059701492537\n",
      "fold: 3 ==> loss: 0.1555143084204252, f1_score: 0.5509761388286335\n",
      "fold: 4 ==> loss: 0.15704243865426, f1_score: 0.554268148498463\n",
      "fold: 5 ==> loss: 0.1567530727579862, f1_score: 0.5307125307125307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Best_trial = study.best_trial.params\n",
    "preds = np.zeros((len(X_test), 2))\n",
    "kf = KFold(n_splits=5,random_state=48,shuffle=True)\n",
    "loss=[]  # list contains rmse for each fold\n",
    "f1 = []\n",
    "n=0\n",
    "for trn_idx, test_idx in kf.split(np.array(X), np.array(y)):\n",
    "    X_tr,X_val=np.array(X)[trn_idx],np.array(X)[test_idx]\n",
    "    y_tr,y_val=np.array(y)[trn_idx],np.array(y)[test_idx]\n",
    "    model = LGBMClassifier(**Best_trial)\n",
    "    model.fit(X_tr,y_tr,eval_set=[(np.array(X_train), np.array(y_train)), (np.array(X_val), np.array(y_val))],early_stopping_rounds=100,eval_metric = 'logloss', verbose=False)\n",
    "#     preds+=model.predict_proba(X_test)\n",
    "    loss.append(log_loss(y_val, model.predict_proba(X_val)))\n",
    "    f1.append(f1_score(y_val, model.predict(X_val)))\n",
    "    print(f\"fold: {n+1} ==> loss: {loss[n]}, f1_score: {f1[n]}\")\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f342886e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 1 ==> loss: 0.16151938553027528, f1_score: 0.9654734838470894\n",
      "fold: 2 ==> loss: 0.15427763369706507, f1_score: 0.9674964936886397\n",
      "fold: 3 ==> loss: 0.1555143084204252, f1_score: 0.9672646764245928\n",
      "fold: 4 ==> loss: 0.15704243865426, f1_score: 0.9668303155079272\n",
      "fold: 5 ==> loss: 0.1567530727579862, f1_score: 0.9664841721064084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Best_trial = study.best_trial.params\n",
    "preds = np.zeros((len(X_test), 2))\n",
    "kf = KFold(n_splits=5,random_state=48,shuffle=True)\n",
    "loss=[]  # list contains rmse for each fold\n",
    "f1 = []\n",
    "n=0\n",
    "for trn_idx, test_idx in kf.split(np.array(X), np.array(y)):\n",
    "    X_tr,X_val=np.array(X)[trn_idx],np.array(X)[test_idx]\n",
    "    y_tr,y_val=np.array(y)[trn_idx],np.array(y)[test_idx]\n",
    "    model = LGBMClassifier(**Best_trial)\n",
    "    model.fit(X_tr,y_tr,eval_set=[(np.array(X_train), np.array(y_train)), (np.array(X_val), np.array(y_val))],early_stopping_rounds=100,eval_metric = 'logloss', verbose=False)\n",
    "#     preds+=model.predict_proba(X_test)\n",
    "    loss.append(log_loss(y_val, model.predict_proba(X_val)))\n",
    "    f1.append(f1_score(y_val, model.predict(X_val), pos_label=0))\n",
    "    print(f\"fold: {n+1} ==> loss: {loss[n]}, f1_score: {f1[n]}\")\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cadaac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 1 (0.041)\n",
      "2. feature 0 (0.038)\n",
      "3. feature 2 (0.035)\n",
      "4. feature 3 (0.033)\n",
      "5. feature 69 (0.030)\n",
      "6. feature 79 (0.016)\n",
      "7. feature 19 (0.014)\n",
      "8. feature 24 (0.014)\n",
      "9. feature 9 (0.013)\n",
      "10. feature 54 (0.012)\n",
      "11. feature 49 (0.012)\n",
      "12. feature 4 (0.010)\n",
      "13. feature 82 (0.010)\n",
      "14. feature 44 (0.010)\n",
      "15. feature 87 (0.009)\n",
      "16. feature 94 (0.009)\n",
      "17. feature 99 (0.009)\n",
      "18. feature 81 (0.009)\n",
      "19. feature 92 (0.009)\n",
      "20. feature 91 (0.009)\n",
      "21. feature 107 (0.009)\n",
      "22. feature 39 (0.009)\n",
      "23. feature 97 (0.009)\n",
      "24. feature 57 (0.009)\n",
      "25. feature 27 (0.009)\n",
      "26. feature 89 (0.009)\n",
      "27. feature 96 (0.009)\n",
      "28. feature 106 (0.009)\n",
      "29. feature 104 (0.009)\n",
      "30. feature 56 (0.009)\n",
      "31. feature 71 (0.009)\n",
      "32. feature 86 (0.009)\n",
      "33. feature 25 (0.009)\n",
      "34. feature 52 (0.008)\n",
      "35. feature 17 (0.008)\n",
      "36. feature 26 (0.008)\n",
      "37. feature 37 (0.008)\n",
      "38. feature 95 (0.008)\n",
      "39. feature 77 (0.008)\n",
      "40. feature 80 (0.008)\n",
      "41. feature 51 (0.008)\n",
      "42. feature 85 (0.008)\n",
      "43. feature 34 (0.008)\n",
      "44. feature 84 (0.008)\n",
      "45. feature 50 (0.008)\n",
      "46. feature 90 (0.008)\n",
      "47. feature 42 (0.008)\n",
      "48. feature 78 (0.008)\n",
      "49. feature 16 (0.008)\n",
      "50. feature 62 (0.008)\n",
      "51. feature 12 (0.008)\n",
      "52. feature 36 (0.008)\n",
      "53. feature 32 (0.008)\n",
      "54. feature 14 (0.008)\n",
      "55. feature 21 (0.008)\n",
      "56. feature 72 (0.008)\n",
      "57. feature 29 (0.008)\n",
      "58. feature 76 (0.008)\n",
      "59. feature 22 (0.008)\n",
      "60. feature 55 (0.008)\n",
      "61. feature 88 (0.007)\n",
      "62. feature 15 (0.007)\n",
      "63. feature 108 (0.007)\n",
      "64. feature 41 (0.007)\n",
      "65. feature 7 (0.007)\n",
      "66. feature 11 (0.007)\n",
      "67. feature 47 (0.007)\n",
      "68. feature 75 (0.007)\n",
      "69. feature 70 (0.007)\n",
      "70. feature 59 (0.007)\n",
      "71. feature 102 (0.007)\n",
      "72. feature 105 (0.007)\n",
      "73. feature 35 (0.007)\n",
      "74. feature 74 (0.007)\n",
      "75. feature 61 (0.007)\n",
      "76. feature 46 (0.007)\n",
      "77. feature 53 (0.007)\n",
      "78. feature 18 (0.007)\n",
      "79. feature 93 (0.007)\n",
      "80. feature 31 (0.007)\n",
      "81. feature 63 (0.007)\n",
      "82. feature 33 (0.007)\n",
      "83. feature 68 (0.007)\n",
      "84. feature 6 (0.007)\n",
      "85. feature 101 (0.007)\n",
      "86. feature 28 (0.007)\n",
      "87. feature 40 (0.007)\n",
      "88. feature 60 (0.007)\n",
      "89. feature 67 (0.007)\n",
      "90. feature 20 (0.007)\n",
      "91. feature 10 (0.007)\n",
      "92. feature 45 (0.007)\n",
      "93. feature 43 (0.007)\n",
      "94. feature 30 (0.007)\n",
      "95. feature 66 (0.007)\n",
      "96. feature 5 (0.006)\n",
      "97. feature 100 (0.006)\n",
      "98. feature 64 (0.006)\n",
      "99. feature 38 (0.006)\n",
      "100. feature 65 (0.006)\n",
      "101. feature 83 (0.006)\n",
      "102. feature 98 (0.006)\n",
      "103. feature 58 (0.006)\n",
      "104. feature 103 (0.006)\n",
      "105. feature 13 (0.006)\n",
      "106. feature 8 (0.005)\n",
      "107. feature 48 (0.005)\n",
      "108. feature 23 (0.005)\n",
      "109. feature 73 (0.003)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"{}. feature {} ({:.3f})\".format(f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.title(\"Feature ranking:\")\n",
    "# plt.bar(range(X.shape[1]), importances[indices],\n",
    "#         color='r', yerr=std[indices], align='center', alpha=0.3)\n",
    "# plt.xticks(range(X.shape[1]), X.columns[indices], rotation=45)\n",
    "# plt.xlim([-1, X.shape[1]])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lazy",
   "language": "python",
   "name": "lazy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
