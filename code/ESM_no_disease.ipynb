{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63749906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from transformers import ESMTokenizer, ESMModel\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c340a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'NUM_WORKERS':4,\n",
    "    'EPITOPE_MAX_LEN':72,\n",
    "    'EPOCHS':20,\n",
    "    'LEARNING_RATE':5e-5,\n",
    "    'BATCH_SIZE':512,\n",
    "    'THRESHOLD':0.5,   # 기본적으로 0.5로 사용하지만 data impalance가 심할 경우 더 큰 값을 사용하기도 한다.\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5954aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b2c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ESMTokenizer.from_pretrained(\"facebook/esm-1b\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9df25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_protein_feature(seq):\n",
    "    protein_feature = []\n",
    "    protein_feature.append(ProteinAnalysis(seq).isoelectric_point())\n",
    "    protein_feature.append(ProteinAnalysis(seq).aromaticity())\n",
    "    protein_feature.append(ProteinAnalysis(seq).gravy())\n",
    "    protein_feature.append(ProteinAnalysis(seq).instability_index())\n",
    "    return protein_feature\n",
    "    \n",
    "def get_preprocessing(data_type, new_df, tokenizer):\n",
    "    epitope_ids_list = []\n",
    "    epitope_mask_list = []\n",
    "    \n",
    "    protein_features = []\n",
    "#     epitope_features = []\n",
    "        \n",
    "    for epitope, antigen, s_p, e_p in tqdm(zip(new_df['epitope_seq'], new_df['antigen_seq'], new_df['start_position'], new_df['end_position'])):             \n",
    "        protein_features.append(get_protein_feature(antigen))\n",
    "#         epitope_features.append(get_peptide_feature(epitope))\n",
    "                \n",
    "        \n",
    "        epitope_input = tokenizer(epitope, \n",
    "                                  add_special_tokens=True, \n",
    "                                  pad_to_max_length=True, \n",
    "                                  is_split_into_words = True,\n",
    "                                  max_length = CFG['EPITOPE_MAX_LEN'])\n",
    "        epitope_ids = epitope_input['input_ids']\n",
    "        epitope_mask = epitope_input['attention_mask']\n",
    "        \n",
    "        \n",
    "        epitope_ids_list.append(epitope_ids)\n",
    "        epitope_mask_list.append(epitope_mask)\n",
    "\n",
    "    \n",
    "    label_list = None\n",
    "    if data_type != 'test':\n",
    "        label_list = []\n",
    "        for label in new_df['label']:\n",
    "            label_list.append(label)\n",
    "    print(f'{data_type} dataframe preprocessing was done.')\n",
    "    \n",
    "    \n",
    "    return epitope_ids_list, epitope_mask_list, protein_features, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3964b1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/hyungseok/.local/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "152648it [03:42, 687.30it/s]\n",
      "65it [00:00, 611.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38163it [00:55, 684.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataframe preprocessing was done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "train, val = train_test_split(train, train_size=0.8, random_state=12)\n",
    "\n",
    "train_epitope_ids_list, train_epitope_mask_list, train_antigen_feature, train_label_list = get_preprocessing('train', train, tokenizer)\n",
    "val_epitope_ids_list, val_epitope_mask_list, val_antigen_feature, val_label_list = get_preprocessing('val', val, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f172f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = pd.DataFrame(train_antigen_feature)\n",
    "val_feature = pd.DataFrame(val_antigen_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1613326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_card_split(df, cols, n=11):\n",
    "    \"\"\"\n",
    "    Splits categorical columns into 2 lists based on cardinality (i.e # of unique values)\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "        DataFrame from which the cardinality of the columns is calculated.\n",
    "    cols : list-like\n",
    "        Categorical columns to list\n",
    "    n : int, optional (default=11)\n",
    "        The value of 'n' will be used to split columns.\n",
    "    Returns\n",
    "    -------\n",
    "    card_low : list-like\n",
    "        Columns with cardinality < n\n",
    "    card_high : list-like\n",
    "        Columns with cardinality >= n\n",
    "    \"\"\"\n",
    "    cond = df[cols].nunique() > n\n",
    "    card_high = cols[cond]\n",
    "    card_low = cols[~cond]\n",
    "    return card_low, card_high\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer_low = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"encoding\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer_high = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        # 'OrdianlEncoder' Raise a ValueError when encounters an unknown value. Check https://github.com/scikit-learn/scikit-learn/pull/13423\n",
    "        (\"encoding\", OrdinalEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "numeric_features = train_feature.select_dtypes(include=[np.number]).columns\n",
    "categorical_features = train_feature.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "categorical_low, categorical_high = get_card_split(\n",
    "    train_feature, categorical_features\n",
    ")\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_transformer, numeric_features),\n",
    "        (\"categorical_low\", categorical_transformer_low, categorical_low),\n",
    "        (\"categorical_high\", categorical_transformer_high, categorical_high),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_feature = preprocessor.fit_transform(train_feature)\n",
    "val_feature = preprocessor.transform(val_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b74960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 epitope_ids_list, \n",
    "                 epitope_mask_list,   \n",
    "                 input_feature,\n",
    "                 label_list):\n",
    "        self.epitope_ids_list = epitope_ids_list\n",
    "        self.epitope_mask_list = epitope_mask_list\n",
    "        self.input_feature = input_feature\n",
    "        self.label_list = label_list\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        self.epitope_ids = self.epitope_ids_list[index]\n",
    "        self.epitope_mask = self.epitope_mask_list[index]\n",
    "        input_feature = self.input_feature[index]\n",
    "              \n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            self.label = self.label_list[index]\n",
    "            return torch.tensor(self.epitope_ids), torch.tensor(self.epitope_mask), torch.FloatTensor(input_feature), self.label\n",
    "        else:\n",
    "            return torch.tensor(self.epitope_ids), torch.tensor(self.epitope_mask), torch.FloatTensor(input_feature)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.epitope_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c609220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_epitope_ids_list, \n",
    "                              train_epitope_mask_list,    \n",
    "                              train_feature,\n",
    "                              train_label_list)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "\n",
    "val_dataset = CustomDataset(val_epitope_ids_list, \n",
    "                            val_epitope_mask_list,\n",
    "                            val_feature,\n",
    "                            val_label_list)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be1c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, pretrained_model='facebook/esm-1b'):\n",
    "        super(TransformerModel, self).__init__()              \n",
    "        # Transformer                \n",
    "        self.esm = ESMModel.from_pretrained(pretrained_model)          \n",
    "        \n",
    "\n",
    "        in_channels = 1280 + 4\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.Linear(in_channels, in_channels//4),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.BatchNorm1d(in_channels//4),\n",
    "            nn.Linear(in_channels//4, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, epitope_x1, epitope_x2, input_feature):\n",
    "        \n",
    "        # Get Embedding Vector\n",
    "        epitope = self.esm(input_ids=epitope_x1, attention_mask=epitope_x2).last_hidden_state\n",
    "        \n",
    "        epitope = epitope[:, 0, :]\n",
    "        \n",
    "        # Feature Concat -> Binary Classifier                \n",
    "        \n",
    "        x = torch.cat([epitope, input_feature], axis=-1)        \n",
    "        \n",
    "        x = self.classifier(x).view(-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9239a3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WeightedFocalLoss(nn.Module):\n",
    "    \"Non weighted version of Focal Loss\"\n",
    "    def __init__(self, alpha=.25, gamma=2):\n",
    "        \n",
    "        super(WeightedFocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor([alpha, 1-alpha]).to(device)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):        \n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        targets = targets.type(torch.long)\n",
    "        at = self.alpha.gather(0, targets.data.view(-1))\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed5f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        criterion = WeightedFocalLoss().to(device)\n",
    "        for epitope_ids_list, epitope_mask_list, input_feature, label in tqdm(iter(train_loader)):            \n",
    "            \n",
    "            \n",
    "            \n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "            \n",
    "            input_feature = input_feature.to(device)\n",
    "\n",
    "            label = label.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()            \n",
    "            \n",
    "            output = model(epitope_ids_list, epitope_mask_list, input_feature)\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                    \n",
    "        val_loss, val_f1, val_acc, val_precision, val_recall = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] Val F1 : [{val_f1:.5f}] Val acc : [{val_acc:.5f}] Val precision : [{val_precision:.5f}] Val recall : [{val_recall:.5f}]')\n",
    "        \n",
    "        if best_val_f1 < val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "#             torch.save(model.module.state_dict(), './feature_transformer_best_model.pth', _use_new_zipfile_serialization=False)\n",
    "            torch.save(model.module.state_dict(), '/DAS_Storage4/hyungseok/esm_final.pth', _use_new_zipfile_serialization=False)\n",
    "            print('Model Saved.')\n",
    "    return best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0982f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    pred_proba_label = []\n",
    "    true_label = []\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for epitope_ids_list, epitope_mask_list, input_feature, label in tqdm(iter(val_loader)):\n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "            \n",
    "            input_feature = input_feature.to(device)\n",
    "            \n",
    "            label = label.float().to(device)\n",
    "            \n",
    "            model_pred = model(epitope_ids_list, epitope_mask_list, input_feature)\n",
    "            \n",
    "            loss = criterion(model_pred, label)\n",
    "            model_pred = torch.sigmoid(model_pred).to('cpu')\n",
    "            \n",
    "            pred_proba_label += model_pred.tolist()\n",
    "            true_label += label.to('cpu').tolist()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    pred_label = np.where(np.array(pred_proba_label)>CFG['THRESHOLD'], 1, 0)\n",
    "    val_f1 = f1_score(true_label, pred_label, average='macro')\n",
    "    acc = accuracy_score(true_label, pred_label)\n",
    "    precision = precision_score(true_label, pred_label)\n",
    "    recall = recall_score(true_label, pred_label)\n",
    "    return np.mean(val_loss), val_f1, acc, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef06b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb8c160a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm-1b were not used when initializing ESMModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing ESMModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ESMModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ESMModel were not initialized from the model checkpoint at facebook/esm-1b and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel()\n",
    "model = nn.DataParallel(model, device_ids=[0,1,2])\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000*CFG['EPOCHS'], eta_min=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8b57e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, para in model.named_parameters():\n",
    "    if name in [\n",
    "'module.esm.encoder.layer.31.attention.self.query.weight',\n",
    "'module.esm.encoder.layer.31.attention.self.query.bias',\n",
    "'module.esm.encoder.layer.31.attention.self.key.weight',\n",
    "'module.esm.encoder.layer.31.attention.self.key.bias',\n",
    "'module.esm.encoder.layer.31.attention.self.value.weight',\n",
    "'module.esm.encoder.layer.31.attention.self.value.bias',\n",
    "'module.esm.encoder.layer.31.attention.output.dense.weight',\n",
    "'module.esm.encoder.layer.31.attention.output.dense.bias',\n",
    "'module.esm.encoder.layer.31.attention.LayerNorm.weight',\n",
    "'module.esm.encoder.layer.31.attention.LayerNorm.bias',\n",
    "'module.esm.encoder.layer.31.intermediate.dense.weight',\n",
    "'module.esm.encoder.layer.31.intermediate.dense.bias',\n",
    "'module.esm.encoder.layer.31.output.dense.weight',\n",
    "'module.esm.encoder.layer.31.output.dense.bias',\n",
    "'module.esm.encoder.layer.31.LayerNorm.weight',\n",
    "'module.esm.encoder.layer.31.LayerNorm.bias',\n",
    "'module.esm.encoder.layer.32.attention.self.query.weight',\n",
    "'module.esm.encoder.layer.32.attention.self.query.bias',\n",
    "'module.esm.encoder.layer.32.attention.self.key.weight',\n",
    "'module.esm.encoder.layer.32.attention.self.key.bias',\n",
    "'module.esm.encoder.layer.32.attention.self.value.weight',\n",
    "'module.esm.encoder.layer.32.attention.self.value.bias',\n",
    "'module.esm.encoder.layer.32.attention.output.dense.weight',\n",
    "'module.esm.encoder.layer.32.attention.output.dense.bias',\n",
    "'module.esm.encoder.layer.32.attention.LayerNorm.weight',\n",
    "'module.esm.encoder.layer.32.attention.LayerNorm.bias',\n",
    "'module.esm.encoder.layer.32.intermediate.dense.weight',\n",
    "'module.esm.encoder.layer.32.intermediate.dense.bias',\n",
    "'module.esm.encoder.layer.32.output.dense.weight',\n",
    "'module.esm.encoder.layer.32.output.dense.bias',\n",
    "'module.esm.encoder.layer.32.LayerNorm.weight',\n",
    "'module.esm.encoder.layer.32.LayerNorm.bias',\n",
    "'module.esm.encoder.emb_layer_norm_after.weight',\n",
    "'module.esm.encoder.emb_layer_norm_after.bias',\n",
    "'module.esm.pooler.dense.weight',\n",
    "'module.esm.pooler.dense.bias',\n",
    "'module.classifier.1.weight',\n",
    "'module.classifier.1.bias',\n",
    "'module.classifier.2.weight',\n",
    "'module.classifier.2.bias',\n",
    "'module.classifier.4.weight',\n",
    "'module.classifier.4.bias',\n",
    "'module.classifier.5.weight',\n",
    "'module.classifier.5.bias']:\n",
    "        para.requires_grad = True\n",
    "        \n",
    "    else:\n",
    "        para.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc58f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/299 [00:21<12:50,  2.63s/it]"
     ]
    }
   ],
   "source": [
    "best_score = train(model, optimizer, train_loader, val_loader, scheduler, device)\n",
    "print(f'Best Validation F1 Score : [{best_score:.5f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_epitope_ids_list, test_epitope_mask_list, test_antigen_feature, test_label_list= get_preprocessing('test', test_df, tokenizer)\n",
    "test_feature = preprocessor.transform(test_antigen_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_epitope_ids_list, \n",
    "                             test_epitope_mask_list, \n",
    "                             test_feature,\n",
    "                             test_label_list)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424654c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    pred_proba_label = []\n",
    "    with torch.no_grad():\n",
    "        for epitope_ids_list, epitope_mask_list, protein_features in tqdm(iter(test_loader)):\n",
    "            epitope_ids_list = epitope_ids_list.to(device)\n",
    "            epitope_mask_list = epitope_mask_list.to(device)\n",
    "\n",
    "            protein_features = protein_features.to(device)\n",
    "            \n",
    "            model_pred = model(epitope_ids_list, epitope_mask_list, protein_features)\n",
    "            model_pred = torch.sigmoid(model_pred).to('cpu')\n",
    "            \n",
    "            pred_proba_label += model_pred.tolist()\n",
    "    \n",
    "    pred_label = np.where(np.array(pred_proba_label)>CFG['THRESHOLD'], 1, 0)\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd81e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference(model, test_loader, device)\n",
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit['label'] = preds\n",
    "\n",
    "submit.to_csv('submission/esm_final.csv', index=False)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9aa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80978aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bec11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26c702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81290a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
